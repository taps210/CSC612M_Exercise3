log/default/ADCprecision=5/batch_size=8/c2cVari=0.003/cellBit=5/d2dVari=0.0/decreasing_lr=200,250/detect=0/grad_scale=1/inference=0/max_level=32/nonlinearityLTD=1.46/nonlinearityLTP=1.75/onoffratio=10/seed=117/subArray=128/t=0/target=0/type=cifar10/v=0/vari=0/wl_activate=8/wl_error=8/wl_grad=5/wl_weight=5
=================FLAGS==================
type: cifar10
batch_size: 8
epochs: 2
grad_scale: 1
seed: 117
log_interval: 100
test_interval: 1
logdir: log/default/ADCprecision=5/batch_size=8/c2cVari=0.003/cellBit=5/d2dVari=0.0/decreasing_lr=200,250/detect=0/grad_scale=1/inference=0/max_level=32/nonlinearityLTD=1.46/nonlinearityLTP=1.75/onoffratio=10/seed=117/subArray=128/t=0/target=0/type=cifar10/v=0/vari=0/wl_activate=8/wl_error=8/wl_grad=5/wl_weight=5
decreasing_lr: 200,250
wl_weight: 5
wl_grad: 5
wl_activate: 8
wl_error: 8
inference: 0
onoffratio: 10
cellBit: 5
subArray: 128
ADCprecision: 5
vari: 0
t: 0
v: 0
detect: 0
target: 0
nonlinearityLTP: 1.75
nonlinearityLTD: 1.46
max_level: 32
d2dVari: 0.0
c2cVari: 0.003
========================================
Building CIFAR-10 data loader with 0 workers
Files already downloaded and verified
Files already downloaded and verified
fan_in     75, float_limit 0.200000, quant limit 1.5, scale 8
fan_in    150, float_limit 0.141421, quant limit 1.5, scale 8
fan_in    400, float_limit 0.086603, quant limit 1.5, scale 16
fan_in    120, float_limit 0.158114, quant limit 1.5, scale 8
fan_in     84, float_limit 0.188982, quant limit 1.5, scale 8
Sequential(
  (0): QConv2d(3, 6, kernel_size=(5, 5), stride=(1, 1), bias=False)
  (1): ReLU()
  (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (3): QConv2d(6, 16, kernel_size=(5, 5), stride=(1, 1), bias=False)
  (4): ReLU()
  (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
)
Sequential(
  (0): QLinear(in_features=400, out_features=120, bias=False)
  (1): ReLU(inplace=True)
  (2): QLinear(in_features=120, out_features=84, bias=False)
  (3): ReLU(inplace=True)
  (4): QLinear(in_features=84, out_features=10, bias=False)
)
decreasing_lr: [200, 250]
training phase
/home/anoobis/csc604m/DNN_NeuroSim_V2.1/Training_pytorch/utee/wage_quantizer.py:17: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1724789115405/work/torch/csrc/tensor/python_tensor.cpp:78.)
  r = torch.cuda.FloatTensor(*x.size()).uniform_()
Train Epoch: 0 [800/50000] Loss: 3.659353 Acc: 0.0000 lr: 1.00e+00
Train Epoch: 0 [1600/50000] Loss: 3.488306 Acc: 0.2500 lr: 1.00e+00
Train Epoch: 0 [2400/50000] Loss: 3.582684 Acc: 0.1250 lr: 1.00e+00
Train Epoch: 0 [3200/50000] Loss: 3.494888 Acc: 0.2500 lr: 1.00e+00
Train Epoch: 0 [4000/50000] Loss: 3.421754 Acc: 0.2500 lr: 1.00e+00
Train Epoch: 0 [4800/50000] Loss: 3.461689 Acc: 0.1250 lr: 1.00e+00
Train Epoch: 0 [5600/50000] Loss: 3.606668 Acc: 0.2500 lr: 1.00e+00
Train Epoch: 0 [6400/50000] Loss: 3.626491 Acc: 0.0000 lr: 1.00e+00
Train Epoch: 0 [7200/50000] Loss: 3.296609 Acc: 0.5000 lr: 1.00e+00
Train Epoch: 0 [8000/50000] Loss: 3.440591 Acc: 0.1250 lr: 1.00e+00
Train Epoch: 0 [8800/50000] Loss: 3.557947 Acc: 0.1250 lr: 1.00e+00
Train Epoch: 0 [9600/50000] Loss: 3.547534 Acc: 0.2500 lr: 1.00e+00
Train Epoch: 0 [10400/50000] Loss: 3.418169 Acc: 0.1250 lr: 1.00e+00
Train Epoch: 0 [11200/50000] Loss: 3.510495 Acc: 0.1250 lr: 1.00e+00
Train Epoch: 0 [12000/50000] Loss: 3.544132 Acc: 0.1250 lr: 1.00e+00
Train Epoch: 0 [12800/50000] Loss: 3.515876 Acc: 0.0000 lr: 1.00e+00
Train Epoch: 0 [13600/50000] Loss: 3.461585 Acc: 0.1250 lr: 1.00e+00
Train Epoch: 0 [14400/50000] Loss: 3.401194 Acc: 0.3750 lr: 1.00e+00
Train Epoch: 0 [15200/50000] Loss: 3.702807 Acc: 0.0000 lr: 1.00e+00
Train Epoch: 0 [16000/50000] Loss: 3.408695 Acc: 0.5000 lr: 1.00e+00
Train Epoch: 0 [16800/50000] Loss: 3.412558 Acc: 0.2500 lr: 1.00e+00
Train Epoch: 0 [17600/50000] Loss: 3.534588 Acc: 0.1250 lr: 1.00e+00
Train Epoch: 0 [18400/50000] Loss: 3.532098 Acc: 0.1250 lr: 1.00e+00
Train Epoch: 0 [19200/50000] Loss: 3.847458 Acc: 0.1250 lr: 1.00e+00
Train Epoch: 0 [20000/50000] Loss: 3.537096 Acc: 0.1250 lr: 1.00e+00
Train Epoch: 0 [20800/50000] Loss: 3.389482 Acc: 0.3750 lr: 1.00e+00
Train Epoch: 0 [21600/50000] Loss: 3.564404 Acc: 0.2500 lr: 1.00e+00
Train Epoch: 0 [22400/50000] Loss: 3.828823 Acc: 0.0000 lr: 1.00e+00
Train Epoch: 0 [23200/50000] Loss: 3.514631 Acc: 0.1250 lr: 1.00e+00
Train Epoch: 0 [24000/50000] Loss: 3.531958 Acc: 0.0000 lr: 1.00e+00
Train Epoch: 0 [24800/50000] Loss: 3.648848 Acc: 0.0000 lr: 1.00e+00
Train Epoch: 0 [25600/50000] Loss: 3.336843 Acc: 0.1250 lr: 1.00e+00
Train Epoch: 0 [26400/50000] Loss: 3.401929 Acc: 0.2500 lr: 1.00e+00
Train Epoch: 0 [27200/50000] Loss: 3.343676 Acc: 0.1250 lr: 1.00e+00
Train Epoch: 0 [28000/50000] Loss: 3.700751 Acc: 0.1250 lr: 1.00e+00
Train Epoch: 0 [28800/50000] Loss: 3.597308 Acc: 0.0000 lr: 1.00e+00
Train Epoch: 0 [29600/50000] Loss: 3.557916 Acc: 0.0000 lr: 1.00e+00
Train Epoch: 0 [30400/50000] Loss: 3.483058 Acc: 0.1250 lr: 1.00e+00
Train Epoch: 0 [31200/50000] Loss: 3.321435 Acc: 0.2500 lr: 1.00e+00
Train Epoch: 0 [32000/50000] Loss: 3.390018 Acc: 0.2500 lr: 1.00e+00
Train Epoch: 0 [32800/50000] Loss: 3.546219 Acc: 0.5000 lr: 1.00e+00
Train Epoch: 0 [33600/50000] Loss: 3.454550 Acc: 0.1250 lr: 1.00e+00
Train Epoch: 0 [34400/50000] Loss: 3.540169 Acc: 0.1250 lr: 1.00e+00
Train Epoch: 0 [35200/50000] Loss: 3.476248 Acc: 0.0000 lr: 1.00e+00
Train Epoch: 0 [36000/50000] Loss: 3.694218 Acc: 0.1250 lr: 1.00e+00
Train Epoch: 0 [36800/50000] Loss: 3.607936 Acc: 0.0000 lr: 1.00e+00
Train Epoch: 0 [37600/50000] Loss: 3.508546 Acc: 0.0000 lr: 1.00e+00
Train Epoch: 0 [38400/50000] Loss: 3.501479 Acc: 0.0000 lr: 1.00e+00
Train Epoch: 0 [39200/50000] Loss: 3.719933 Acc: 0.0000 lr: 1.00e+00
Train Epoch: 0 [40000/50000] Loss: 3.293224 Acc: 0.2500 lr: 1.00e+00
Train Epoch: 0 [40800/50000] Loss: 3.543562 Acc: 0.1250 lr: 1.00e+00
Train Epoch: 0 [41600/50000] Loss: 3.541560 Acc: 0.0000 lr: 1.00e+00
Train Epoch: 0 [42400/50000] Loss: 3.753606 Acc: 0.1250 lr: 1.00e+00
Train Epoch: 0 [43200/50000] Loss: 3.598367 Acc: 0.3750 lr: 1.00e+00
Train Epoch: 0 [44000/50000] Loss: 3.794456 Acc: 0.1250 lr: 1.00e+00
Train Epoch: 0 [44800/50000] Loss: 3.558657 Acc: 0.2500 lr: 1.00e+00
Train Epoch: 0 [45600/50000] Loss: 3.504321 Acc: 0.0000 lr: 1.00e+00
Train Epoch: 0 [46400/50000] Loss: 3.162607 Acc: 0.6250 lr: 1.00e+00
Train Epoch: 0 [47200/50000] Loss: 3.643257 Acc: 0.1250 lr: 1.00e+00
Train Epoch: 0 [48000/50000] Loss: 3.724566 Acc: 0.2500 lr: 1.00e+00
Train Epoch: 0 [48800/50000] Loss: 3.506273 Acc: 0.1250 lr: 1.00e+00
Train Epoch: 0 [49600/50000] Loss: 3.514013 Acc: 0.0000 lr: 1.00e+00
Elapsed 151.36s, 151.36 s/epoch, 0.02 s/batch, ets 151.36s
weight distribution
[-0.05471245 -0.31068614 -0.21702899 -0.25309038  0.20188196  0.43075475
  0.62844729  0.73196393  0.72463745  0.81514484]
delta distribution
[ 7.36111123e-03  1.04166672e-03  2.60416673e-05  3.10019859e-05
 -7.44047647e-05  2.61760168e-02  1.23277344e-02  2.67599151e-03
  1.39170932e-03  3.73657281e-03]
testing phase
	Epoch 0 Test set: Average loss: 3.4771, Accuracy: 1573/10000 (16%)
Saving model to /home/anoobis/csc604m/DNN_NeuroSim_V2.1/Training_pytorch/log/default/ADCprecision=5/batch_size=8/c2cVari=0.003/cellBit=5/d2dVari=0.0/decreasing_lr=200,250/detect=0/grad_scale=1/inference=0/max_level=32/nonlinearityLTD=1.46/nonlinearityLTP=1.75/onoffratio=10/seed=117/subArray=128/t=0/target=0/type=cifar10/v=0/vari=0/wl_activate=8/wl_error=8/wl_grad=5/wl_weight=5/best-0.pth
------------------------------ FloorPlan --------------------------------

Tile and PE size are optimized to maximize memory utilization ( = memory mapped by synapse / total memory on chip)

Desired Conventional Mapped Tile Storage Size: 256x256
Desired Conventional PE Storage Size: 128x128
Desired Novel Mapped Tile Storage Size: 1x128x128
User-defined SubArray Size: 64x64

----------------- # of tile used for each layer -----------------
layer1: 1
layer2: 1
layer3: 1
layer4: 1
layer5: 1

----------------- Speed-up of each layer ------------------
layer1: 8
layer2: 4
layer3: 8
layer4: 1
layer5: 2

----------------- Utilization of each layer ------------------
layer1: 0.0549316
layer2: 0.146484
layer3: 0.234375
layer4: 0.615234
layer5: 0.102539
Memory Utilization of Whole Chip: 23.0713 % 

---------------------------- FloorPlan Done ------------------------------



-------------------------------------- Hardware Performance --------------------------------------
-------------------- Estimation of Layer 1 ----------------------
layer1's readLatency of Forward is: 5.75861e+09ns
layer1's readDynamicEnergy of Forward is: 1.2683e+10pJ
layer1's readLatency of Activation Gradient is: 7.28501e+07ns
layer1's readDynamicEnergy of Activation Gradient is: 0pJ
layer1's readLatency of Weight Gradient is: 4.48511e+09ns
layer1's readDynamicEnergy of Weight Gradient is: 5.19926e+10pJ
layer1's writeLatency of Weight Update is: 7.77671e+06ns
layer1's writeDynamicEnergy of Weight Update is: 126231pJ
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
layer1's PEAK readLatency of Forward is: 2.49772e+09ns
layer1's PEAK readDynamicEnergy of Forward is: 5.76624e+09pJ
layer1's PEAK readLatency of Activation Gradient is: 7.28501e+07ns
layer1's PEAK readDynamicEnergy of Activation Gradient is: 0pJ
layer1's PEAK readLatency of Weight Gradient is: 3.14556e+09ns
layer1's PEAK readDynamicEnergy of Weight Gradient is: 4.58143e+10pJ
layer1's PEAK writeLatency of Weight Update is: 7.76968e+06ns
layer1's PEAK writeDynamicEnergy of Weight Update is: 123104pJ
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
layer1's leakagePower is: 0.602228uW
layer1's leakageEnergy is: 1.40475e+07pJ

************************ Breakdown of Latency and Dynamic Energy *************************

----------- ADC (or S/As and precharger for SRAM) readLatency is : 2.1952e+09ns
----------- Accumulation Circuits (subarray level: adders, shiftAdds; PE/Tile/Global level: accumulation units) readLatency is : 1.2397e+08ns
----------- Synaptic Array w/o ADC (Forward + Activate Gradient) readLatency is : 1.78555e+08ns
----------- Buffer buffer latency is: 4.38337e+09ns
----------- Interconnect latency is: 2.1831e+08ns
----------- Weight Gradient Calculation readLatency is : 3.14556e+09ns
----------- Weight Update writeLatency is : 7.76968e+06ns
----------- DRAM data transfer Latency is : 694105ns
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
----------- ADC (or S/As and precharger for SRAM) readDynamicEnergy is : 8.68435e+08pJ
----------- Accumulation Circuits (subarray level: adders, shiftAdds; PE/Tile/Global level: accumulation units) readDynamicEnergy is : 3.82509e+09pJ
----------- Synaptic Array w/o ADC (Forward + Activate Gradient) readDynamicEnergy is : 1.07271e+09pJ
----------- Buffer readDynamicEnergy is: 5.36926e+08pJ
----------- Interconnect readDynamicEnergy is: 3.36105e+09pJ
----------- Weight Gradient Calculation readDynamicEnergy is : 4.58143e+10pJ
----------- Weight Update writeDynamicEnergy is : 123104pJ
----------- DRAM data transfer Energy is : 4.11466e+10pJ

************************ Breakdown of Latency and Dynamic Energy *************************

-------------------- Estimation of Layer 2 ----------------------
layer2's readLatency of Forward is: 1.5579e+09ns
layer2's readDynamicEnergy of Forward is: 2.73452e+09pJ
layer2's readLatency of Activation Gradient is: 1.57304e+09ns
layer2's readDynamicEnergy of Activation Gradient is: 2.73643e+09pJ
layer2's readLatency of Weight Gradient is: 1.80607e+09ns
layer2's readDynamicEnergy of Weight Gradient is: 3.6918e+10pJ
layer2's writeLatency of Weight Update is: 2.38009e+06ns
layer2's writeDynamicEnergy of Weight Update is: 35242.3pJ
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
layer2's PEAK readLatency of Forward is: 7.16601e+08ns
layer2's PEAK readDynamicEnergy of Forward is: 1.12605e+09pJ
layer2's PEAK readLatency of Activation Gradient is: 7.31735e+08ns
layer2's PEAK readDynamicEnergy of Activation Gradient is: 1.12796e+09pJ
layer2's PEAK readLatency of Weight Gradient is: 1.4213e+09ns
layer2's PEAK readDynamicEnergy of Weight Gradient is: 2.12595e+10pJ
layer2's PEAK writeLatency of Weight Update is: 2.34261e+06ns
layer2's PEAK writeDynamicEnergy of Weight Update is: 18564.8pJ
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
layer2's leakagePower is: 0.787958uW
layer2's leakageEnergy is: 9.86819e+06pJ

************************ Breakdown of Latency and Dynamic Energy *************************

----------- ADC (or S/As and precharger for SRAM) readLatency is : 1.12e+09ns
----------- Accumulation Circuits (subarray level: adders, shiftAdds; PE/Tile/Global level: accumulation units) readLatency is : 2.2135e+08ns
----------- Synaptic Array w/o ADC (Forward + Activate Gradient) readLatency is : 1.06986e+08ns
----------- Buffer buffer latency is: 1.96255e+09ns
----------- Interconnect latency is: 1.12037e+08ns
----------- Weight Gradient Calculation readLatency is : 1.4213e+09ns
----------- Weight Update writeLatency is : 2.34261e+06ns
----------- DRAM data transfer Latency is : 438316ns
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
----------- ADC (or S/As and precharger for SRAM) readDynamicEnergy is : 5.25296e+08pJ
----------- Accumulation Circuits (subarray level: adders, shiftAdds; PE/Tile/Global level: accumulation units) readDynamicEnergy is : 1.3689e+09pJ
----------- Synaptic Array w/o ADC (Forward + Activate Gradient) readDynamicEnergy is : 3.59812e+08pJ
----------- Buffer readDynamicEnergy is: 2.94688e+08pJ
----------- Interconnect readDynamicEnergy is: 1.77356e+09pJ
----------- Weight Gradient Calculation readDynamicEnergy is : 2.12595e+10pJ
----------- Weight Update writeDynamicEnergy is : 18564.8pJ
----------- DRAM data transfer Energy is : 2.59834e+10pJ

************************ Breakdown of Latency and Dynamic Energy *************************

-------------------- Estimation of Layer 3 ----------------------
layer3's readLatency of Forward is: 3.12711e+08ns
layer3's readDynamicEnergy of Forward is: 5.47618e+08pJ
layer3's readLatency of Activation Gradient is: 3.12419e+08ns
layer3's readDynamicEnergy of Activation Gradient is: 5.30457e+08pJ
layer3's readLatency of Weight Gradient is: 3.17944e+08ns
layer3's readDynamicEnergy of Weight Gradient is: 1.46194e+10pJ
layer3's writeLatency of Weight Update is: 29983.8ns
layer3's writeDynamicEnergy of Weight Update is: 18916.9pJ
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
layer3's PEAK readLatency of Forward is: 7.95539e+07ns
layer3's PEAK readDynamicEnergy of Forward is: 2.10662e+08pJ
layer3's PEAK readLatency of Activation Gradient is: 7.92626e+07ns
layer3's PEAK readDynamicEnergy of Activation Gradient is: 1.93502e+08pJ
layer3's PEAK readLatency of Weight Gradient is: 2.0816e+08ns
layer3's PEAK readDynamicEnergy of Weight Gradient is: 2.5291e+09pJ
layer3's PEAK writeLatency of Weight Update is: 0ns
layer3's PEAK writeDynamicEnergy of Weight Update is: 5574.93pJ
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
layer3's leakagePower is: 0.787958uW
layer3's leakageEnergy is: 1.9703e+06pJ

************************ Breakdown of Latency and Dynamic Energy *************************

----------- ADC (or S/As and precharger for SRAM) readLatency is : 1.4e+08ns
----------- Accumulation Circuits (subarray level: adders, shiftAdds; PE/Tile/Global level: accumulation units) readLatency is : 3.2602e+06ns
----------- Synaptic Array w/o ADC (Forward + Activate Gradient) readLatency is : 1.55563e+07ns
----------- Buffer buffer latency is: 5.53036e+08ns
----------- Interconnect latency is: 2.88528e+07ns
----------- Weight Gradient Calculation readLatency is : 2.0816e+08ns
----------- Weight Update writeLatency is : 0ns
----------- DRAM data transfer Latency is : 225789ns
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
----------- ADC (or S/As and precharger for SRAM) readDynamicEnergy is : 1.42929e+08pJ
----------- Accumulation Circuits (subarray level: adders, shiftAdds; PE/Tile/Global level: accumulation units) readDynamicEnergy is : 1.9341e+08pJ
----------- Synaptic Array w/o ADC (Forward + Activate Gradient) readDynamicEnergy is : 6.7825e+07pJ
----------- Buffer readDynamicEnergy is: 1.33416e+08pJ
----------- Interconnect readDynamicEnergy is: 4.18634e+08pJ
----------- Weight Gradient Calculation readDynamicEnergy is : 2.5291e+09pJ
----------- Weight Update writeDynamicEnergy is : 5574.93pJ
----------- DRAM data transfer Energy is : 1.33848e+10pJ

************************ Breakdown of Latency and Dynamic Energy *************************

-------------------- Estimation of Layer 4 ----------------------
layer4's readLatency of Forward is: 3.90314e+07ns
layer4's readDynamicEnergy of Forward is: 4.79033e+07pJ
layer4's readLatency of Activation Gradient is: 3.96152e+07ns
layer4's readDynamicEnergy of Activation Gradient is: 4.84804e+07pJ
layer4's readLatency of Weight Gradient is: 1.89093e+08ns
layer4's readDynamicEnergy of Weight Gradient is: 6.3619e+10pJ
layer4's writeLatency of Weight Update is: 244349ns
layer4's writeDynamicEnergy of Weight Update is: 17677.4pJ
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
layer4's PEAK readLatency of Forward is: 2.55236e+07ns
layer4's PEAK readDynamicEnergy of Forward is: 1.74752e+07pJ
layer4's PEAK readLatency of Activation Gradient is: 2.61074e+07ns
layer4's PEAK readDynamicEnergy of Activation Gradient is: 1.80524e+07pJ
layer4's PEAK readLatency of Weight Gradient is: 4.47901e+06ns
layer4's PEAK readDynamicEnergy of Weight Gradient is: 6.01543e+08pJ
layer4's PEAK writeLatency of Weight Update is: 165524ns
layer4's PEAK writeDynamicEnergy of Weight Update is: 3473.15pJ
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
layer4's leakagePower is: 0.230125uW
layer4's leakageEnergy is: 72394.1pJ

************************ Breakdown of Latency and Dynamic Energy *************************

----------- ADC (or S/As and precharger for SRAM) readLatency is : 4.48e+07ns
----------- Accumulation Circuits (subarray level: adders, shiftAdds; PE/Tile/Global level: accumulation units) readLatency is : 2.53e+06ns
----------- Synaptic Array w/o ADC (Forward + Activate Gradient) readLatency is : 4.30103e+06ns
----------- Buffer buffer latency is: 2.25342e+08ns
----------- Interconnect latency is: 991122ns
----------- Weight Gradient Calculation readLatency is : 4.47901e+06ns
----------- Weight Update writeLatency is : 165524ns
----------- DRAM data transfer Latency is : 1.062e+06ns
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
----------- ADC (or S/As and precharger for SRAM) readDynamicEnergy is : 1.43899e+07pJ
----------- Accumulation Circuits (subarray level: adders, shiftAdds; PE/Tile/Global level: accumulation units) readDynamicEnergy is : 1.64836e+07pJ
----------- Synaptic Array w/o ADC (Forward + Activate Gradient) readDynamicEnergy is : 4.65413e+06pJ
----------- Buffer readDynamicEnergy is: 1.6056e+08pJ
----------- Interconnect readDynamicEnergy is: 1.2e+07pJ
----------- Weight Gradient Calculation readDynamicEnergy is : 6.01543e+08pJ
----------- Weight Update writeDynamicEnergy is : 3473.15pJ
----------- DRAM data transfer Energy is : 6.29554e+10pJ

************************ Breakdown of Latency and Dynamic Energy *************************

-------------------- Estimation of Layer 5 ----------------------
layer5's readLatency of Forward is: 1.70786e+07ns
layer5's readDynamicEnergy of Forward is: 1.80089e+07pJ
layer5's readLatency of Activation Gradient is: 1.7383e+07ns
layer5's readDynamicEnergy of Activation Gradient is: 1.83944e+07pJ
layer5's readLatency of Weight Gradient is: 1.92524e+07ns
layer5's readDynamicEnergy of Weight Gradient is: 5.29526e+09pJ
layer5's writeLatency of Weight Update is: 83588.1ns
layer5's writeDynamicEnergy of Weight Update is: 5180.37pJ
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
layer5's PEAK readLatency of Forward is: 1.27493e+07ns
layer5's PEAK readDynamicEnergy of Forward is: 5.93359e+06pJ
layer5's PEAK readLatency of Activation Gradient is: 1.30537e+07ns
layer5's PEAK readDynamicEnergy of Activation Gradient is: 6.31911e+06pJ
layer5's PEAK readLatency of Weight Gradient is: 2.15838e+06ns
layer5's PEAK readDynamicEnergy of Weight Gradient is: 4.07341e+07pJ
layer5's PEAK writeLatency of Weight Update is: 77019.4ns
layer5's PEAK writeDynamicEnergy of Weight Update is: 1998.34pJ
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
layer5's leakagePower is: 0.230125uW
layer5's leakageEnergy is: 31721.8pJ

************************ Breakdown of Latency and Dynamic Energy *************************

----------- ADC (or S/As and precharger for SRAM) readLatency is : 2.24e+07ns
----------- Accumulation Circuits (subarray level: adders, shiftAdds; PE/Tile/Global level: accumulation units) readLatency is : 1.265e+06ns
----------- Synaptic Array w/o ADC (Forward + Activate Gradient) readLatency is : 2.13801e+06ns
----------- Buffer buffer latency is: 2.65811e+07ns
----------- Interconnect latency is: 396558ns
----------- Weight Gradient Calculation readLatency is : 2.15838e+06ns
----------- Weight Update writeLatency is : 77019.4ns
----------- DRAM data transfer Latency is : 89368.4ns
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
----------- ADC (or S/As and precharger for SRAM) readDynamicEnergy is : 1.22316e+06pJ
----------- Accumulation Circuits (subarray level: adders, shiftAdds; PE/Tile/Global level: accumulation units) readDynamicEnergy is : 8.74715e+06pJ
----------- Synaptic Array w/o ADC (Forward + Activate Gradient) readDynamicEnergy is : 2.28239e+06pJ
----------- Buffer readDynamicEnergy is: 2.19648e+07pJ
----------- Interconnect readDynamicEnergy is: 5.99313e+06pJ
----------- Weight Gradient Calculation readDynamicEnergy is : 4.07341e+07pJ
----------- Weight Update writeDynamicEnergy is : 1998.34pJ
----------- DRAM data transfer Energy is : 5.29776e+09pJ

************************ Breakdown of Latency and Dynamic Energy *************************

------------------------------ Summary --------------------------------

ChipArea : 1.32129e+06um^2
Chip total CIM (Forward+Activation Gradient) array : 3758.1um^2
Total IC Area on chip (Global and Tile/PE local): 226211um^2
Total ADC (or S/As and precharger for SRAM) Area on chip : 271000um^2
Total Accumulation Circuits (subarray level: adders, shiftAdds; PE/Tile/Global level: accumulation units) on chip : 166808um^2
Other Peripheries (e.g. decoders, mux, switchmatrix, buffers, pooling and activation units) : 260803um^2
Weight Gradient Calculation : 392714um^2

-----------------------------------Chip layer-by-layer Estimation---------------------------------
Chip readLatency of Forward (per epoch) is: 7.68533e+09ns
Chip readDynamicEnergy of Forward (per epoch) is: 1.60311e+10pJ
Chip readLatency of Activation Gradient (per epoch) is: 2.0153e+09ns
Chip readDynamicEnergy of Activation Gradient (per epoch) is: 3.33377e+09pJ
Chip readLatency of Weight Gradient (per epoch) is: 6.81747e+09ns
Chip readDynamicEnergy of Weight Gradient (per epoch) is: 1.72444e+11pJ
Chip writeLatency of Weight Update (per epoch) is: 1.05147e+07ns
Chip writeDynamicEnergy of Weight Update (per epoch) is: 203248pJ
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Chip total Latency (per epoch) is: 1.65286e+10ns
Chip total Energy (per epoch) is: 1.91809e+11pJ
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Chip PEAK readLatency of Forward (per epoch) is: 3.33215e+09ns
Chip PEAK readDynamicEnergy of Forward (per epoch) is: 7.12636e+09pJ
Chip PEAK readLatency of Activation Gradient (per epoch) is: 9.23009e+08ns
Chip PEAK readDynamicEnergy of Activation Gradient (per epoch) is: 1.34583e+09pJ
Chip PEAK readLatency of Weight Gradient (per epoch) is: 4.78165e+09ns
Chip PEAK readDynamicEnergy of Weight Gradient (per epoch) is: 7.02451e+10pJ
Chip PEAK writeLatency of Weight Update (per epoch) is: 1.03548e+07ns
Chip PEAK writeDynamicEnergy of Weight Update (per epoch) is: 152715pJ
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Chip PEAK total Latency (per epoch) is: 9.04717e+09ns
Chip PEAK total Energy (per epoch) is: 7.87175e+10pJ
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Chip leakage Energy is: 2.59901e+07pJ
Chip leakage Power is: 2.63839uW

************************ Breakdown of Latency and Dynamic Energy *************************

----------- ADC (or S/As and precharger for SRAM) readLatency is : 3.5224e+09ns
----------- Accumulation Circuits (subarray level: adders, shiftAdds; PE/Tile/Global level: accumulation units) readLatency is : 3.52375e+08ns
----------- Synaptic Array w/o ADC (Forward + Activate Gradient) readLatency is : 3.07536e+08ns
----------- Buffer readLatency is: 7.15088e+09ns
----------- Interconnect readLatency is: 3.60587e+08ns
----------- Weight Gradient Calculation readLatency is : 4.78165e+09ns
----------- Weight Update writeLatency is : 1.03548e+07ns
----------- DRAM data transfer Latency is : 2.50958e+06ns
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
----------- ADC (or S/As and precharger for SRAM) readDynamicEnergy is : 1.55227e+09pJ
----------- Accumulation Circuits (subarray level: adders, shiftAdds; PE/Tile/Global level: accumulation units) readDynamicEnergy is : 5.41263e+09pJ
----------- Synaptic Array w/o ADC (Forward + Activate Gradient) readDynamicEnergy is : 1.50729e+09pJ
----------- Buffer readDynamicEnergy is: 1.14755e+09pJ
----------- Interconnect readDynamicEnergy is: 5.57124e+09pJ
----------- Weight Gradient Calculation readDynamicEnergy is : 7.02451e+10pJ
----------- Weight Update writeDynamicEnergy is : 152715pJ
----------- DRAM data transfer DynamicEnergy is : 1.48768e+11pJ

************************ Breakdown of Latency and Dynamic Energy *************************


-----------------------------------Chip layer-by-layer Performance---------------------------------
Energy Efficiency TOPS/W: 1.30819
Throughput TOPS: 0.0151831
Throughput FPS: 0.0605011
--------------------------------------------------------------------------
Peak Energy Efficiency TOPS/W: 3.18806
Peak Throughput TOPS: 0.0277386
Peak Throughput FPS: 0.110532
-------------------------------------- Hardware Performance Done --------------------------------------

------------------------------ Simulation Performance --------------------------------
Total Run-time of NeuroSim: 0 seconds
------------------------------ Simulation Performance --------------------------------
training phase
Train Epoch: 1 [800/50000] Loss: 3.402027 Acc: 0.1250 lr: 1.00e+00
Train Epoch: 1 [1600/50000] Loss: 3.408064 Acc: 0.1250 lr: 1.00e+00
Train Epoch: 1 [2400/50000] Loss: 3.363067 Acc: 0.2500 lr: 1.00e+00
Train Epoch: 1 [3200/50000] Loss: 3.204823 Acc: 0.3750 lr: 1.00e+00
Train Epoch: 1 [4000/50000] Loss: 3.487660 Acc: 0.1250 lr: 1.00e+00
Train Epoch: 1 [4800/50000] Loss: 3.143798 Acc: 0.5000 lr: 1.00e+00
Train Epoch: 1 [5600/50000] Loss: 3.686157 Acc: 0.2500 lr: 1.00e+00
Train Epoch: 1 [6400/50000] Loss: 3.638776 Acc: 0.0000 lr: 1.00e+00
Train Epoch: 1 [7200/50000] Loss: 3.678415 Acc: 0.0000 lr: 1.00e+00
Train Epoch: 1 [8000/50000] Loss: 3.357390 Acc: 0.2500 lr: 1.00e+00
Train Epoch: 1 [8800/50000] Loss: 3.620543 Acc: 0.1250 lr: 1.00e+00
Train Epoch: 1 [9600/50000] Loss: 3.411593 Acc: 0.2500 lr: 1.00e+00
Train Epoch: 1 [10400/50000] Loss: 3.837119 Acc: 0.0000 lr: 1.00e+00
Train Epoch: 1 [11200/50000] Loss: 3.525424 Acc: 0.1250 lr: 1.00e+00
Train Epoch: 1 [12000/50000] Loss: 3.608920 Acc: 0.3750 lr: 1.00e+00
Train Epoch: 1 [12800/50000] Loss: 3.332823 Acc: 0.2500 lr: 1.00e+00
Train Epoch: 1 [13600/50000] Loss: 3.366760 Acc: 0.2500 lr: 1.00e+00
Train Epoch: 1 [14400/50000] Loss: 3.275543 Acc: 0.2500 lr: 1.00e+00
Train Epoch: 1 [15200/50000] Loss: 3.579147 Acc: 0.0000 lr: 1.00e+00
Train Epoch: 1 [16000/50000] Loss: 3.502574 Acc: 0.2500 lr: 1.00e+00
Train Epoch: 1 [16800/50000] Loss: 3.500282 Acc: 0.0000 lr: 1.00e+00
Train Epoch: 1 [17600/50000] Loss: 3.398948 Acc: 0.2500 lr: 1.00e+00
Train Epoch: 1 [18400/50000] Loss: 3.574751 Acc: 0.1250 lr: 1.00e+00
Train Epoch: 1 [19200/50000] Loss: 3.391541 Acc: 0.5000 lr: 1.00e+00
Train Epoch: 1 [20000/50000] Loss: 3.454568 Acc: 0.0000 lr: 1.00e+00
Train Epoch: 1 [20800/50000] Loss: 3.307403 Acc: 0.5000 lr: 1.00e+00
Train Epoch: 1 [21600/50000] Loss: 3.453345 Acc: 0.1250 lr: 1.00e+00
Train Epoch: 1 [22400/50000] Loss: 3.493499 Acc: 0.2500 lr: 1.00e+00
Train Epoch: 1 [23200/50000] Loss: 3.643480 Acc: 0.1250 lr: 1.00e+00
Train Epoch: 1 [24000/50000] Loss: 3.518039 Acc: 0.2500 lr: 1.00e+00
Train Epoch: 1 [24800/50000] Loss: 3.532765 Acc: 0.2500 lr: 1.00e+00
Train Epoch: 1 [25600/50000] Loss: 3.479645 Acc: 0.1250 lr: 1.00e+00
Train Epoch: 1 [26400/50000] Loss: 3.756609 Acc: 0.1250 lr: 1.00e+00
Train Epoch: 1 [27200/50000] Loss: 3.444291 Acc: 0.1250 lr: 1.00e+00
Train Epoch: 1 [28000/50000] Loss: 3.297948 Acc: 0.1250 lr: 1.00e+00
Train Epoch: 1 [28800/50000] Loss: 3.511435 Acc: 0.2500 lr: 1.00e+00
Train Epoch: 1 [29600/50000] Loss: 3.498959 Acc: 0.1250 lr: 1.00e+00
Train Epoch: 1 [30400/50000] Loss: 3.346054 Acc: 0.2500 lr: 1.00e+00
Train Epoch: 1 [31200/50000] Loss: 3.303454 Acc: 0.3750 lr: 1.00e+00
Train Epoch: 1 [32000/50000] Loss: 3.629858 Acc: 0.0000 lr: 1.00e+00
Train Epoch: 1 [32800/50000] Loss: 3.781051 Acc: 0.0000 lr: 1.00e+00
Train Epoch: 1 [33600/50000] Loss: 3.795366 Acc: 0.0000 lr: 1.00e+00
Train Epoch: 1 [34400/50000] Loss: 3.766426 Acc: 0.1250 lr: 1.00e+00
Train Epoch: 1 [35200/50000] Loss: 3.628546 Acc: 0.0000 lr: 1.00e+00
Train Epoch: 1 [36000/50000] Loss: 4.015483 Acc: 0.2500 lr: 1.00e+00
Train Epoch: 1 [36800/50000] Loss: 3.438594 Acc: 0.1250 lr: 1.00e+00
Train Epoch: 1 [37600/50000] Loss: 3.429815 Acc: 0.1250 lr: 1.00e+00
Train Epoch: 1 [38400/50000] Loss: 3.600137 Acc: 0.1250 lr: 1.00e+00
Train Epoch: 1 [39200/50000] Loss: 3.485166 Acc: 0.2500 lr: 1.00e+00
Train Epoch: 1 [40000/50000] Loss: 3.237397 Acc: 0.5000 lr: 1.00e+00
Train Epoch: 1 [40800/50000] Loss: 3.256300 Acc: 0.2500 lr: 1.00e+00
Train Epoch: 1 [41600/50000] Loss: 3.340854 Acc: 0.2500 lr: 1.00e+00
Train Epoch: 1 [42400/50000] Loss: 3.753903 Acc: 0.1250 lr: 1.00e+00
Train Epoch: 1 [43200/50000] Loss: 3.556803 Acc: 0.2500 lr: 1.00e+00
Train Epoch: 1 [44000/50000] Loss: 3.557115 Acc: 0.2500 lr: 1.00e+00
Train Epoch: 1 [44800/50000] Loss: 3.814550 Acc: 0.1250 lr: 1.00e+00
Train Epoch: 1 [45600/50000] Loss: 3.523380 Acc: 0.1250 lr: 1.00e+00
Train Epoch: 1 [46400/50000] Loss: 3.753219 Acc: 0.1250 lr: 1.00e+00
Train Epoch: 1 [47200/50000] Loss: 3.414745 Acc: 0.2500 lr: 1.00e+00
Train Epoch: 1 [48000/50000] Loss: 3.227395 Acc: 0.3750 lr: 1.00e+00
Train Epoch: 1 [48800/50000] Loss: 3.479671 Acc: 0.2500 lr: 1.00e+00
Train Epoch: 1 [49600/50000] Loss: 3.515237 Acc: 0.1250 lr: 1.00e+00
Elapsed 299.60s, 149.80 s/epoch, 0.02 s/batch, ets 0.00s
weight distribution
[-0.1187704  -0.35607198 -0.23247853 -0.26531857  0.18817349  0.47395566
  0.58376259  0.72811276  0.72142178  0.80970269]
delta distribution
[-9.72222246e-04  5.98958344e-04 -2.34375002e-05 -2.48015876e-05
  0.00000000e+00  2.68541146e-02  1.18870391e-02  3.42322164e-03
  1.24484452e-03  3.05150473e-03]
testing phase
	Epoch 1 Test set: Average loss: 3.4873, Accuracy: 1716/10000 (17%)
Removing old model /home/anoobis/csc604m/DNN_NeuroSim_V2.1/Training_pytorch/log/default/ADCprecision=5/batch_size=8/c2cVari=0.003/cellBit=5/d2dVari=0.0/decreasing_lr=200,250/detect=0/grad_scale=1/inference=0/max_level=32/nonlinearityLTD=1.46/nonlinearityLTP=1.75/onoffratio=10/seed=117/subArray=128/t=0/target=0/type=cifar10/v=0/vari=0/wl_activate=8/wl_error=8/wl_grad=5/wl_weight=5/best-0.pth
Saving model to /home/anoobis/csc604m/DNN_NeuroSim_V2.1/Training_pytorch/log/default/ADCprecision=5/batch_size=8/c2cVari=0.003/cellBit=5/d2dVari=0.0/decreasing_lr=200,250/detect=0/grad_scale=1/inference=0/max_level=32/nonlinearityLTD=1.46/nonlinearityLTP=1.75/onoffratio=10/seed=117/subArray=128/t=0/target=0/type=cifar10/v=0/vari=0/wl_activate=8/wl_error=8/wl_grad=5/wl_weight=5/best-1.pth
------------------------------ FloorPlan --------------------------------

Tile and PE size are optimized to maximize memory utilization ( = memory mapped by synapse / total memory on chip)

Desired Conventional Mapped Tile Storage Size: 256x256
Desired Conventional PE Storage Size: 128x128
Desired Novel Mapped Tile Storage Size: 1x128x128
User-defined SubArray Size: 64x64

----------------- # of tile used for each layer -----------------
layer1: 1
layer2: 1
layer3: 1
layer4: 1
layer5: 1

----------------- Speed-up of each layer ------------------
layer1: 8
layer2: 4
layer3: 8
layer4: 1
layer5: 2

----------------- Utilization of each layer ------------------
layer1: 0.0549316
layer2: 0.146484
layer3: 0.234375
layer4: 0.615234
layer5: 0.102539
Memory Utilization of Whole Chip: 23.0713 % 

---------------------------- FloorPlan Done ------------------------------



-------------------------------------- Hardware Performance --------------------------------------
-------------------- Estimation of Layer 1 ----------------------
layer1's readLatency of Forward is: 5.75861e+09ns
layer1's readDynamicEnergy of Forward is: 1.26772e+10pJ
layer1's readLatency of Activation Gradient is: 7.28501e+07ns
layer1's readDynamicEnergy of Activation Gradient is: 0pJ
layer1's readLatency of Weight Gradient is: 4.48511e+09ns
layer1's readDynamicEnergy of Weight Gradient is: 5.19926e+10pJ
layer1's writeLatency of Weight Update is: 8.72716e+06ns
layer1's writeDynamicEnergy of Weight Update is: 149288pJ
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
layer1's PEAK readLatency of Forward is: 2.49772e+09ns
layer1's PEAK readDynamicEnergy of Forward is: 5.7604e+09pJ
layer1's PEAK readLatency of Activation Gradient is: 7.28501e+07ns
layer1's PEAK readDynamicEnergy of Activation Gradient is: 0pJ
layer1's PEAK readLatency of Weight Gradient is: 3.14556e+09ns
layer1's PEAK readDynamicEnergy of Weight Gradient is: 4.58143e+10pJ
layer1's PEAK writeLatency of Weight Update is: 8.72013e+06ns
layer1's PEAK writeDynamicEnergy of Weight Update is: 146161pJ
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
layer1's leakagePower is: 0.602228uW
layer1's leakageEnergy is: 1.40475e+07pJ

************************ Breakdown of Latency and Dynamic Energy *************************

----------- ADC (or S/As and precharger for SRAM) readLatency is : 2.1952e+09ns
----------- Accumulation Circuits (subarray level: adders, shiftAdds; PE/Tile/Global level: accumulation units) readLatency is : 1.2397e+08ns
----------- Synaptic Array w/o ADC (Forward + Activate Gradient) readLatency is : 1.78555e+08ns
----------- Buffer buffer latency is: 4.38337e+09ns
----------- Interconnect latency is: 2.1831e+08ns
----------- Weight Gradient Calculation readLatency is : 3.14556e+09ns
----------- Weight Update writeLatency is : 8.72013e+06ns
----------- DRAM data transfer Latency is : 694105ns
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
----------- ADC (or S/As and precharger for SRAM) readDynamicEnergy is : 8.62594e+08pJ
----------- Accumulation Circuits (subarray level: adders, shiftAdds; PE/Tile/Global level: accumulation units) readDynamicEnergy is : 3.82509e+09pJ
----------- Synaptic Array w/o ADC (Forward + Activate Gradient) readDynamicEnergy is : 1.07271e+09pJ
----------- Buffer readDynamicEnergy is: 5.36926e+08pJ
----------- Interconnect readDynamicEnergy is: 3.36105e+09pJ
----------- Weight Gradient Calculation readDynamicEnergy is : 4.58143e+10pJ
----------- Weight Update writeDynamicEnergy is : 146161pJ
----------- DRAM data transfer Energy is : 4.11466e+10pJ

************************ Breakdown of Latency and Dynamic Energy *************************

-------------------- Estimation of Layer 2 ----------------------
layer2's readLatency of Forward is: 1.5579e+09ns
layer2's readDynamicEnergy of Forward is: 2.74912e+09pJ
layer2's readLatency of Activation Gradient is: 1.57304e+09ns
layer2's readDynamicEnergy of Activation Gradient is: 2.73945e+09pJ
layer2's readLatency of Weight Gradient is: 1.80755e+09ns
layer2's readDynamicEnergy of Weight Gradient is: 3.99849e+10pJ
layer2's writeLatency of Weight Update is: 2.55467e+06ns
layer2's writeDynamicEnergy of Weight Update is: 33782.1pJ
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
layer2's PEAK readLatency of Forward is: 7.16601e+08ns
layer2's PEAK readDynamicEnergy of Forward is: 1.14065e+09pJ
layer2's PEAK readLatency of Activation Gradient is: 7.31735e+08ns
layer2's PEAK readDynamicEnergy of Activation Gradient is: 1.13098e+09pJ
layer2's PEAK readLatency of Weight Gradient is: 1.42277e+09ns
layer2's PEAK readDynamicEnergy of Weight Gradient is: 2.43263e+10pJ
layer2's PEAK writeLatency of Weight Update is: 2.51719e+06ns
layer2's PEAK writeDynamicEnergy of Weight Update is: 17104.5pJ
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
layer2's leakagePower is: 0.787958uW
layer2's leakageEnergy is: 9.86819e+06pJ

************************ Breakdown of Latency and Dynamic Energy *************************

----------- ADC (or S/As and precharger for SRAM) readLatency is : 1.12e+09ns
----------- Accumulation Circuits (subarray level: adders, shiftAdds; PE/Tile/Global level: accumulation units) readLatency is : 2.2135e+08ns
----------- Synaptic Array w/o ADC (Forward + Activate Gradient) readLatency is : 1.06986e+08ns
----------- Buffer buffer latency is: 1.96255e+09ns
----------- Interconnect latency is: 1.12037e+08ns
----------- Weight Gradient Calculation readLatency is : 1.42277e+09ns
----------- Weight Update writeLatency is : 2.51719e+06ns
----------- DRAM data transfer Latency is : 438316ns
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
----------- ADC (or S/As and precharger for SRAM) readDynamicEnergy is : 5.38939e+08pJ
----------- Accumulation Circuits (subarray level: adders, shiftAdds; PE/Tile/Global level: accumulation units) readDynamicEnergy is : 1.3689e+09pJ
----------- Synaptic Array w/o ADC (Forward + Activate Gradient) readDynamicEnergy is : 3.63783e+08pJ
----------- Buffer readDynamicEnergy is: 2.94688e+08pJ
----------- Interconnect readDynamicEnergy is: 1.77356e+09pJ
----------- Weight Gradient Calculation readDynamicEnergy is : 2.43263e+10pJ
----------- Weight Update writeDynamicEnergy is : 17104.5pJ
----------- DRAM data transfer Energy is : 2.59834e+10pJ

************************ Breakdown of Latency and Dynamic Energy *************************

-------------------- Estimation of Layer 3 ----------------------
layer3's readLatency of Forward is: 3.12711e+08ns
layer3's readDynamicEnergy of Forward is: 5.4125e+08pJ
layer3's readLatency of Activation Gradient is: 3.12419e+08ns
layer3's readDynamicEnergy of Activation Gradient is: 5.24233e+08pJ
layer3's readLatency of Weight Gradient is: 3.1794e+08ns
layer3's readDynamicEnergy of Weight Gradient is: 1.46193e+10pJ
layer3's writeLatency of Weight Update is: 29983.8ns
layer3's writeDynamicEnergy of Weight Update is: 18916.9pJ
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
layer3's PEAK readLatency of Forward is: 7.95539e+07ns
layer3's PEAK readDynamicEnergy of Forward is: 2.04295e+08pJ
layer3's PEAK readLatency of Activation Gradient is: 7.92626e+07ns
layer3's PEAK readDynamicEnergy of Activation Gradient is: 1.87278e+08pJ
layer3's PEAK readLatency of Weight Gradient is: 2.08155e+08ns
layer3's PEAK readDynamicEnergy of Weight Gradient is: 2.52905e+09pJ
layer3's PEAK writeLatency of Weight Update is: 0ns
layer3's PEAK writeDynamicEnergy of Weight Update is: 5574.93pJ
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
layer3's leakagePower is: 0.787958uW
layer3's leakageEnergy is: 1.9703e+06pJ

************************ Breakdown of Latency and Dynamic Energy *************************

----------- ADC (or S/As and precharger for SRAM) readLatency is : 1.4e+08ns
----------- Accumulation Circuits (subarray level: adders, shiftAdds; PE/Tile/Global level: accumulation units) readLatency is : 3.2602e+06ns
----------- Synaptic Array w/o ADC (Forward + Activate Gradient) readLatency is : 1.55563e+07ns
----------- Buffer buffer latency is: 5.53036e+08ns
----------- Interconnect latency is: 2.88528e+07ns
----------- Weight Gradient Calculation readLatency is : 2.08155e+08ns
----------- Weight Update writeLatency is : 0ns
----------- DRAM data transfer Latency is : 225789ns
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
----------- ADC (or S/As and precharger for SRAM) readDynamicEnergy is : 1.30387e+08pJ
----------- Accumulation Circuits (subarray level: adders, shiftAdds; PE/Tile/Global level: accumulation units) readDynamicEnergy is : 1.9341e+08pJ
----------- Synaptic Array w/o ADC (Forward + Activate Gradient) readDynamicEnergy is : 6.77759e+07pJ
----------- Buffer readDynamicEnergy is: 1.33416e+08pJ
----------- Interconnect readDynamicEnergy is: 4.18634e+08pJ
----------- Weight Gradient Calculation readDynamicEnergy is : 2.52905e+09pJ
----------- Weight Update writeDynamicEnergy is : 5574.93pJ
----------- DRAM data transfer Energy is : 1.33848e+10pJ

************************ Breakdown of Latency and Dynamic Energy *************************

-------------------- Estimation of Layer 4 ----------------------
layer4's readLatency of Forward is: 3.90314e+07ns
layer4's readDynamicEnergy of Forward is: 4.79192e+07pJ
layer4's readLatency of Activation Gradient is: 3.96152e+07ns
layer4's readDynamicEnergy of Activation Gradient is: 4.84843e+07pJ
layer4's readLatency of Weight Gradient is: 1.89095e+08ns
layer4's readDynamicEnergy of Weight Gradient is: 6.3619e+10pJ
layer4's writeLatency of Weight Update is: 193345ns
layer4's writeDynamicEnergy of Weight Update is: 17104.5pJ
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
layer4's PEAK readLatency of Forward is: 2.55236e+07ns
layer4's PEAK readDynamicEnergy of Forward is: 1.74912e+07pJ
layer4's PEAK readLatency of Activation Gradient is: 2.61074e+07ns
layer4's PEAK readDynamicEnergy of Activation Gradient is: 1.80563e+07pJ
layer4's PEAK readLatency of Weight Gradient is: 4.48134e+06ns
layer4's PEAK readDynamicEnergy of Weight Gradient is: 6.01549e+08pJ
layer4's PEAK writeLatency of Weight Update is: 114520ns
layer4's PEAK writeDynamicEnergy of Weight Update is: 2900.32pJ
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
layer4's leakagePower is: 0.230125uW
layer4's leakageEnergy is: 72394.1pJ

************************ Breakdown of Latency and Dynamic Energy *************************

----------- ADC (or S/As and precharger for SRAM) readLatency is : 4.48e+07ns
----------- Accumulation Circuits (subarray level: adders, shiftAdds; PE/Tile/Global level: accumulation units) readLatency is : 2.53e+06ns
----------- Synaptic Array w/o ADC (Forward + Activate Gradient) readLatency is : 4.30103e+06ns
----------- Buffer buffer latency is: 2.25342e+08ns
----------- Interconnect latency is: 991122ns
----------- Weight Gradient Calculation readLatency is : 4.48134e+06ns
----------- Weight Update writeLatency is : 114520ns
----------- DRAM data transfer Latency is : 1.062e+06ns
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
----------- ADC (or S/As and precharger for SRAM) readDynamicEnergy is : 1.44056e+07pJ
----------- Accumulation Circuits (subarray level: adders, shiftAdds; PE/Tile/Global level: accumulation units) readDynamicEnergy is : 1.64836e+07pJ
----------- Synaptic Array w/o ADC (Forward + Activate Gradient) readDynamicEnergy is : 4.65827e+06pJ
----------- Buffer readDynamicEnergy is: 1.6056e+08pJ
----------- Interconnect readDynamicEnergy is: 1.2e+07pJ
----------- Weight Gradient Calculation readDynamicEnergy is : 6.01549e+08pJ
----------- Weight Update writeDynamicEnergy is : 2900.32pJ
----------- DRAM data transfer Energy is : 6.29554e+10pJ

************************ Breakdown of Latency and Dynamic Energy *************************

-------------------- Estimation of Layer 5 ----------------------
layer5's readLatency of Forward is: 1.70786e+07ns
layer5's readDynamicEnergy of Forward is: 1.80075e+07pJ
layer5's readLatency of Activation Gradient is: 1.7383e+07ns
layer5's readDynamicEnergy of Activation Gradient is: 1.8393e+07pJ
layer5's readLatency of Weight Gradient is: 1.92524e+07ns
layer5's readDynamicEnergy of Weight Gradient is: 5.29526e+09pJ
layer5's writeLatency of Weight Update is: 70078ns
layer5's writeDynamicEnergy of Weight Update is: 4043.31pJ
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
layer5's PEAK readLatency of Forward is: 1.27493e+07ns
layer5's PEAK readDynamicEnergy of Forward is: 5.93222e+06pJ
layer5's PEAK readLatency of Activation Gradient is: 1.30537e+07ns
layer5's PEAK readDynamicEnergy of Activation Gradient is: 6.31774e+06pJ
layer5's PEAK readLatency of Weight Gradient is: 2.15838e+06ns
layer5's PEAK readDynamicEnergy of Weight Gradient is: 4.07341e+07pJ
layer5's PEAK writeLatency of Weight Update is: 63509.2ns
layer5's PEAK writeDynamicEnergy of Weight Update is: 1429.81pJ
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
layer5's leakagePower is: 0.230125uW
layer5's leakageEnergy is: 31721.8pJ

************************ Breakdown of Latency and Dynamic Energy *************************

----------- ADC (or S/As and precharger for SRAM) readLatency is : 2.24e+07ns
----------- Accumulation Circuits (subarray level: adders, shiftAdds; PE/Tile/Global level: accumulation units) readLatency is : 1.265e+06ns
----------- Synaptic Array w/o ADC (Forward + Activate Gradient) readLatency is : 2.13801e+06ns
----------- Buffer buffer latency is: 2.65811e+07ns
----------- Interconnect latency is: 396558ns
----------- Weight Gradient Calculation readLatency is : 2.15838e+06ns
----------- Weight Update writeLatency is : 63509.2ns
----------- DRAM data transfer Latency is : 89368.4ns
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
----------- ADC (or S/As and precharger for SRAM) readDynamicEnergy is : 1.22043e+06pJ
----------- Accumulation Circuits (subarray level: adders, shiftAdds; PE/Tile/Global level: accumulation units) readDynamicEnergy is : 8.74715e+06pJ
----------- Synaptic Array w/o ADC (Forward + Activate Gradient) readDynamicEnergy is : 2.28239e+06pJ
----------- Buffer readDynamicEnergy is: 2.19648e+07pJ
----------- Interconnect readDynamicEnergy is: 5.99313e+06pJ
----------- Weight Gradient Calculation readDynamicEnergy is : 4.07341e+07pJ
----------- Weight Update writeDynamicEnergy is : 1429.81pJ
----------- DRAM data transfer Energy is : 5.29776e+09pJ

************************ Breakdown of Latency and Dynamic Energy *************************

------------------------------ Summary --------------------------------

ChipArea : 1.32129e+06um^2
Chip total CIM (Forward+Activation Gradient) array : 3758.1um^2
Total IC Area on chip (Global and Tile/PE local): 226211um^2
Total ADC (or S/As and precharger for SRAM) Area on chip : 271000um^2
Total Accumulation Circuits (subarray level: adders, shiftAdds; PE/Tile/Global level: accumulation units) on chip : 166808um^2
Other Peripheries (e.g. decoders, mux, switchmatrix, buffers, pooling and activation units) : 260803um^2
Weight Gradient Calculation : 392714um^2

-----------------------------------Chip layer-by-layer Estimation---------------------------------
Chip readLatency of Forward (per epoch) is: 7.68533e+09ns
Chip readDynamicEnergy of Forward (per epoch) is: 1.60335e+10pJ
Chip readLatency of Activation Gradient (per epoch) is: 2.0153e+09ns
Chip readDynamicEnergy of Activation Gradient (per epoch) is: 3.33056e+09pJ
Chip readLatency of Weight Gradient (per epoch) is: 6.81894e+09ns
Chip readDynamicEnergy of Weight Gradient (per epoch) is: 1.75511e+11pJ
Chip writeLatency of Weight Update (per epoch) is: 1.15752e+07ns
Chip writeDynamicEnergy of Weight Update (per epoch) is: 223135pJ
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Chip total Latency (per epoch) is: 1.65312e+10ns
Chip total Energy (per epoch) is: 1.94875e+11pJ
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Chip PEAK readLatency of Forward (per epoch) is: 3.33215e+09ns
Chip PEAK readDynamicEnergy of Forward (per epoch) is: 7.12876e+09pJ
Chip PEAK readLatency of Activation Gradient (per epoch) is: 9.23009e+08ns
Chip PEAK readDynamicEnergy of Activation Gradient (per epoch) is: 1.34263e+09pJ
Chip PEAK readLatency of Weight Gradient (per epoch) is: 4.78313e+09ns
Chip PEAK readDynamicEnergy of Weight Gradient (per epoch) is: 7.33119e+10pJ
Chip PEAK writeLatency of Weight Update (per epoch) is: 1.14153e+07ns
Chip PEAK writeDynamicEnergy of Weight Update (per epoch) is: 173171pJ
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Chip PEAK total Latency (per epoch) is: 9.0497e+09ns
Chip PEAK total Energy (per epoch) is: 8.17835e+10pJ
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Chip leakage Energy is: 2.59901e+07pJ
Chip leakage Power is: 2.63839uW

************************ Breakdown of Latency and Dynamic Energy *************************

----------- ADC (or S/As and precharger for SRAM) readLatency is : 3.5224e+09ns
----------- Accumulation Circuits (subarray level: adders, shiftAdds; PE/Tile/Global level: accumulation units) readLatency is : 3.52375e+08ns
----------- Synaptic Array w/o ADC (Forward + Activate Gradient) readLatency is : 3.07536e+08ns
----------- Buffer readLatency is: 7.15088e+09ns
----------- Interconnect readLatency is: 3.60587e+08ns
----------- Weight Gradient Calculation readLatency is : 4.78313e+09ns
----------- Weight Update writeLatency is : 1.14153e+07ns
----------- DRAM data transfer Latency is : 2.50958e+06ns
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
----------- ADC (or S/As and precharger for SRAM) readDynamicEnergy is : 1.54755e+09pJ
----------- Accumulation Circuits (subarray level: adders, shiftAdds; PE/Tile/Global level: accumulation units) readDynamicEnergy is : 5.41263e+09pJ
----------- Synaptic Array w/o ADC (Forward + Activate Gradient) readDynamicEnergy is : 1.51121e+09pJ
----------- Buffer readDynamicEnergy is: 1.14755e+09pJ
----------- Interconnect readDynamicEnergy is: 5.57124e+09pJ
----------- Weight Gradient Calculation readDynamicEnergy is : 7.33119e+10pJ
----------- Weight Update writeDynamicEnergy is : 173171pJ
----------- DRAM data transfer DynamicEnergy is : 1.48768e+11pJ

************************ Breakdown of Latency and Dynamic Energy *************************


-----------------------------------Chip layer-by-layer Performance---------------------------------
Energy Efficiency TOPS/W: 1.28761
Throughput TOPS: 0.0151808
Throughput FPS: 0.0604919
--------------------------------------------------------------------------
Peak Energy Efficiency TOPS/W: 3.06854
Peak Throughput TOPS: 0.0277309
Peak Throughput FPS: 0.110501
-------------------------------------- Hardware Performance Done --------------------------------------

------------------------------ Simulation Performance --------------------------------
Total Run-time of NeuroSim: 0 seconds
------------------------------ Simulation Performance --------------------------------
Total Elapse: 307.41, Best Result: 17.160%
