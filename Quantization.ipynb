{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "3cc44d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      " \"python\": \"3.12.7\",\n",
      " \"pytorch\": \"2.6.0.dev20241112+cu121\",\n",
      " \"device\": \"cuda\",\n",
      " \"cpu\": \"Intel64 Family 6 Model 154 Stepping 3, GenuineIntel\",\n",
      " \"machine\": \"AMD64\",\n",
      " \"platform\": \"Windows-11-10.0.26100-SP0\",\n",
      " \"gpu\": {\n",
      "  \"name\": \"NVIDIA GeForce RTX 4050 Laptop GPU\",\n",
      "  \"compute_capability\": [\n",
      "   8,\n",
      "   9\n",
      "  ],\n",
      "  \"vram_gb\": 6.0\n",
      " }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import os, platform, sys, json, io, time, math, subprocess, threading\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms as T\n",
    "from tinyimagenet import TinyImageNet\n",
    "import pandas as pd\n",
    "import copy\n",
    "\n",
    "torch.manual_seed(0)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(0)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def gpu_specs():\n",
    "    if torch.cuda.is_available():\n",
    "        name = torch.cuda.get_device_name(0)\n",
    "        capability = torch.cuda.get_device_capability(0)\n",
    "        total = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "        return {\"name\": name, \"compute_capability\": capability, \"vram_gb\": round(total,2)}\n",
    "    return None\n",
    "\n",
    "specs = {\n",
    "    \"python\": sys.version.split()[0],\n",
    "    \"pytorch\": torch.__version__,\n",
    "    \"device\": device,\n",
    "    \"cpu\": platform.processor(),\n",
    "    \"machine\": platform.machine(),\n",
    "    \"platform\": platform.platform(),\n",
    "    \"gpu\": gpu_specs(),\n",
    "}\n",
    "print(json.dumps(specs, indent=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b974a1",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "8fe0c1ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TinyImageNet train=100000, val=10000\n"
     ]
    }
   ],
   "source": [
    "# --- Data: TinyImageNet loaders ---\n",
    "train_tfms = T.Compose([\n",
    "    T.Resize((64, 64)),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.ToTensor(),\n",
    "])\n",
    "val_tfms = T.Compose([\n",
    "    T.Resize((64, 64)),\n",
    "    T.ToTensor(),\n",
    "])\n",
    "\n",
    "train_ds = TinyImageNet(Path(\"~/.torchvision/tinyimagenet/\").expanduser(), split=\"train\")\n",
    "val_ds   = TinyImageNet(Path(\"~/.torchvision/tinyimagenet/\").expanduser(), split=\"val\")\n",
    "train_ds.transform = train_tfms\n",
    "val_ds.transform   = val_tfms\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=128, shuffle=True,\n",
    "                          num_workers=0, pin_memory=(device==\"cuda\"))\n",
    "val_loader   = DataLoader(val_ds,   batch_size=256, shuffle=False,\n",
    "                          num_workers=0, pin_memory=(device==\"cuda\"))\n",
    "print(f\"TinyImageNet train={len(train_ds)}, val={len(val_ds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606c3921",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "d4b37a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyLinearModel(torch.nn.Module):\n",
    "    def __init__(self, m=3*64*64, n=200, k=64):\n",
    "        super().__init__()\n",
    "        self.linear1 = torch.nn.Linear(m, k, bias=False)\n",
    "        self.linear2 = torch.nn.Linear(k, n, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() == 4:\n",
    "            x = torch.flatten(x, 1)  # (B, 3*64*64)\n",
    "        x = self.linear1(x)\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf95eef",
   "metadata": {},
   "source": [
    "# Observers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "4837dbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- TorchAO observers & helpers ---\n",
    "from torchao.quantization.granularity import PerAxis, PerTensor\n",
    "from torchao.quantization.observer import AffineQuantizedMinMaxObserver\n",
    "from torchao.quantization.quant_primitives import MappingType\n",
    "from torchao.quantization.quant_api import _replace_with_custom_fn_if_matches_filter\n",
    "\n",
    "# per tensor input activation asymmetric quantization\n",
    "act_obs = AffineQuantizedMinMaxObserver(\n",
    "    MappingType.ASYMMETRIC,\n",
    "    torch.uint8,\n",
    "    granularity=PerTensor(),\n",
    "    eps=torch.finfo(torch.float32).eps,\n",
    "    scale_dtype=torch.float32,\n",
    "    zero_point_dtype=torch.float32,\n",
    ")\n",
    "\n",
    "# per channel weight asymmetric quantization\n",
    "weight_obs = AffineQuantizedMinMaxObserver(\n",
    "    MappingType.ASYMMETRIC,\n",
    "    torch.uint8,\n",
    "    granularity=PerAxis(axis=0),\n",
    "    eps=torch.finfo(torch.float32).eps,\n",
    "    scale_dtype=torch.float32,\n",
    "    zero_point_dtype=torch.float32,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "de96c4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObservedLinear(torch.nn.Linear):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        act_obs: torch.nn.Module,\n",
    "        weight_obs: torch.nn.Module,\n",
    "        bias: bool = True,\n",
    "        device=None,\n",
    "        dtype=None,\n",
    "    ):\n",
    "        super().__init__(in_features, out_features, bias, device, dtype)\n",
    "        self.act_obs = act_obs\n",
    "        self.weight_obs = weight_obs\n",
    "\n",
    "    def forward(self, input: torch.Tensor):\n",
    "        observed_input = self.act_obs(input)\n",
    "        observed_weight = self.weight_obs(self.weight)\n",
    "        return F.linear(observed_input, observed_weight, self.bias)\n",
    "\n",
    "    @classmethod\n",
    "    def from_float(cls, float_linear, act_obs, weight_obs):\n",
    "        observed_linear = cls(\n",
    "            float_linear.in_features,\n",
    "            float_linear.out_features,\n",
    "            act_obs,\n",
    "            weight_obs,\n",
    "            False,\n",
    "            device=float_linear.weight.device,\n",
    "            dtype=float_linear.weight.dtype,\n",
    "        )\n",
    "        observed_linear.weight = float_linear.weight\n",
    "        observed_linear.bias = float_linear.bias\n",
    "        return observed_linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "951839aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_observers_(model, act_obs, weight_obs):\n",
    "    _is_linear = lambda m, fqn: isinstance(m, torch.nn.Linear)\n",
    "\n",
    "    def replacement_fn(m):\n",
    "        copied_act_obs = copy.deepcopy(act_obs)\n",
    "        copied_weight_obs = copy.deepcopy(weight_obs)\n",
    "        return ObservedLinear.from_float(m, copied_act_obs, copied_weight_obs)\n",
    "\n",
    "    _replace_with_custom_fn_if_matches_filter(model, replacement_fn, _is_linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6243fe5",
   "metadata": {},
   "source": [
    "# Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "806ad19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- QuantizedLinear backend (with safe float fallback) ---\n",
    "from torchao.dtypes import to_affine_quantized_intx_static\n",
    "class QuantizedLinear(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        act_obs: torch.nn.Module,\n",
    "        weight_obs: torch.nn.Module,\n",
    "        weight: torch.Tensor,\n",
    "        bias: torch.Tensor,\n",
    "        target_dtype: torch.dtype,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.act_scale, self.act_zero_point = act_obs.calculate_qparams()\n",
    "        weight_scale, weight_zero_point = weight_obs.calculate_qparams()\n",
    "        assert weight.dim() == 2\n",
    "        block_size = (1, weight.shape[1])\n",
    "        self.target_dtype = target_dtype\n",
    "        self.bias = bias\n",
    "        self.qweight = to_affine_quantized_intx_static(\n",
    "            weight, weight_scale, weight_zero_point, block_size, self.target_dtype\n",
    "        )\n",
    "\n",
    "    def forward(self, input: torch.Tensor):\n",
    "        block_size = input.shape\n",
    "        qinput = to_affine_quantized_intx_static(\n",
    "            input,\n",
    "            self.act_scale,\n",
    "            self.act_zero_point,\n",
    "            block_size,\n",
    "            self.target_dtype,\n",
    "        )\n",
    "        return F.linear(qinput, self.qweight, self.bias)\n",
    "\n",
    "    @classmethod\n",
    "    def from_observed(cls, observed_linear, target_dtype):\n",
    "        quantized_linear = cls(\n",
    "            observed_linear.in_features,\n",
    "            observed_linear.out_features,\n",
    "            observed_linear.act_obs,\n",
    "            observed_linear.weight_obs,\n",
    "            observed_linear.weight,\n",
    "            observed_linear.bias,\n",
    "            target_dtype,\n",
    "        )\n",
    "        return quantized_linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "631808be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from torchao.core.config import AOBaseConfig\n",
    "from torchao.quantization import quantize_\n",
    "from torchao.quantization.transform_module import register_quantize_module_handler\n",
    "\n",
    "@dataclass\n",
    "class StaticQuantConfig(AOBaseConfig):\n",
    "    target_dtype: torch.dtype\n",
    "\n",
    "@register_quantize_module_handler(StaticQuantConfig)\n",
    "def _apply_static_quant(module: torch.nn.Module, config: StaticQuantConfig):\n",
    "    return QuantizedLinear.from_observed(module, config.target_dtype)\n",
    "\n",
    "is_observed_linear = lambda m, fqn: isinstance(m, ObservedLinear)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8f5d96",
   "metadata": {},
   "source": [
    "# FP32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "be7c5db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FP32] epoch 1/10 loss=5.4695\n",
      "[FP32] epoch 2/10 loss=4.9832\n",
      "[FP32] epoch 3/10 loss=4.9515\n",
      "[FP32] epoch 4/10 loss=4.9722\n",
      "[FP32] epoch 5/10 loss=4.9785\n",
      "[FP32] epoch 6/10 loss=4.9695\n",
      "[FP32] epoch 7/10 loss=4.9703\n",
      "[FP32] epoch 8/10 loss=4.9595\n",
      "[FP32] epoch 9/10 loss=4.9587\n",
      "[FP32] epoch 10/10 loss=4.9436\n"
     ]
    }
   ],
   "source": [
    "# --- FP32 training baseline ---\n",
    "torch.cuda.empty_cache() if device==\"cuda\" else None\n",
    "\n",
    "def train_float_baseline(epochs=10, lr=2e-3):\n",
    "    model = ToyLinearModel().to(torch.float32).to(device).train()\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    for ep in range(epochs):\n",
    "        running = 0.0\n",
    "        for xb, yb in train_loader:\n",
    "            xb = xb.to(device=device, dtype=torch.float32, non_blocking=(device==\"cuda\"))\n",
    "            yb = yb.to(device)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            logits = model(xb)\n",
    "            loss = loss_fn(logits, yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            running += loss.item()\n",
    "        print(f\"[FP32] epoch {ep+1}/{epochs} loss={running/len(train_loader):.4f}\")\n",
    "    return model.eval()\n",
    "\n",
    "m_fp_trained = train_float_baseline(epochs=10, lr=2e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bae4bd",
   "metadata": {},
   "source": [
    "# PTQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "954663ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PTQ ready.\n"
     ]
    }
   ],
   "source": [
    "# --- PTQ: observe, calibrate, quantize ---\n",
    "m_obs = copy.deepcopy(m_fp_trained).to(torch.float32).to(device).eval()\n",
    "insert_observers_(m_obs, act_obs, weight_obs)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for i, (images, _) in enumerate(train_loader):\n",
    "        images = images.to(device=device, dtype=torch.float32, non_blocking=(device==\"cuda\"))\n",
    "        _ = m_obs(images)\n",
    "        if i >= 50:  # decent min/max calibration\n",
    "            break\n",
    "\n",
    "m_ptq = copy.deepcopy(m_obs)\n",
    "quantize_(m_ptq, StaticQuantConfig(torch.uint8), is_observed_linear)\n",
    "m_ptq.eval().to(device)\n",
    "print(\"PTQ ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2621a1d8",
   "metadata": {},
   "source": [
    "# QAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "14659e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- QAT: fake-quant with STE ---\n",
    "def _aq_clamp_round(x, scale, zp, qmin=0, qmax=255):\n",
    "    q = torch.round(x / scale) + zp\n",
    "    q = torch.clamp(q, qmin, qmax)\n",
    "    return (q - zp) * scale\n",
    "\n",
    "class FakeQLinear(torch.nn.Linear):\n",
    "    def __init__(self, in_features, out_features, act_obs, weight_obs, bias=True, device=None, dtype=None):\n",
    "        super().__init__(in_features, out_features, bias, device, dtype)\n",
    "        self.act_obs = act_obs\n",
    "        self.weight_obs = weight_obs\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "    @classmethod\n",
    "    def from_observed(cls, observed_linear: torch.nn.Linear):\n",
    "        fq = cls(\n",
    "            observed_linear.in_features, observed_linear.out_features,\n",
    "            observed_linear.act_obs, observed_linear.weight_obs,\n",
    "            bias=(observed_linear.bias is not None),\n",
    "            device=observed_linear.weight.device, dtype=observed_linear.weight.dtype,\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            fq.weight.copy_(observed_linear.weight)\n",
    "            if observed_linear.bias is not None:\n",
    "                fq.bias.copy_(observed_linear.bias)\n",
    "        return fq\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x_obs = self.act_obs(x)\n",
    "        w_obs = self.weight_obs(self.weight)\n",
    "        act_scale, act_zp = self.act_obs.calculate_qparams()\n",
    "        w_scale,  w_zp    = self.weight_obs.calculate_qparams()\n",
    "        x_fq = _aq_clamp_round(x_obs, act_scale, act_zp)               # per-tensor acts\n",
    "        w_fq = _aq_clamp_round(w_obs, w_scale.view(-1,1), w_zp.view(-1,1))  # per-channel weights (axis=0)\n",
    "        return F.linear(x_fq, w_fq, self.bias)\n",
    "\n",
    "def replace_observed_with_fakeq_(model):\n",
    "    _is_observed = lambda m, fqn: isinstance(m, ObservedLinear)\n",
    "    def _to_fakeq(m):\n",
    "        return FakeQLinear.from_observed(m)\n",
    "    _replace_with_custom_fn_if_matches_filter(model, _to_fakeq, _is_observed)\n",
    "\n",
    "def replace_fakeq_with_quantized_(model, target_dtype=torch.uint8):\n",
    "    _is_fakeq = lambda m, fqn: isinstance(m, FakeQLinear)\n",
    "    def _to_quant(m):\n",
    "        return QuantizedLinear(\n",
    "            m.in_features, m.out_features, m.act_obs, m.weight_obs, m.weight, m.bias, target_dtype\n",
    "        )\n",
    "    _replace_with_custom_fn_if_matches_filter(model, _to_quant, _is_fakeq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "aa30954f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[QAT] epoch 1/10 loss=4.8713\n",
      "[QAT] epoch 2/10 loss=4.8646\n",
      "[QAT] epoch 3/10 loss=4.8613\n",
      "[QAT] epoch 4/10 loss=4.8567\n",
      "[QAT] epoch 5/10 loss=4.8516\n",
      "[QAT] epoch 6/10 loss=4.8472\n",
      "[QAT] epoch 7/10 loss=4.8443\n",
      "[QAT] epoch 8/10 loss=4.8393\n",
      "[QAT] epoch 9/10 loss=4.8373\n",
      "[QAT] epoch 10/10 loss=4.8360\n",
      "QAT ready.\n"
     ]
    }
   ],
   "source": [
    "# QAT finetune from trained float\n",
    "m_qat_train = copy.deepcopy(m_fp_trained).to(torch.float32).to(device).train()\n",
    "insert_observers_(m_qat_train, act_obs, weight_obs)\n",
    "replace_observed_with_fakeq_(m_qat_train)\n",
    "\n",
    "opt = torch.optim.AdamW(m_qat_train.parameters(), lr=1e-3)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "epochs_qat = 10\n",
    "\n",
    "for ep in range(epochs_qat):\n",
    "    running=0.0\n",
    "    for xb, yb in train_loader:\n",
    "        xb = xb.to(device=device, dtype=torch.float32, non_blocking=(device==\"cuda\"))\n",
    "        yb = yb.to(device)\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        logits = m_qat_train(xb)   # fake-quant forward\n",
    "        loss = loss_fn(logits, yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        running += loss.item()\n",
    "    print(f\"[QAT] epoch {ep+1}/{epochs_qat} loss={running/len(train_loader):.4f}\")\n",
    "\n",
    "m_qat_train.eval()\n",
    "m_qat = copy.deepcopy(m_qat_train).eval()\n",
    "replace_fakeq_with_quantized_(m_qat, target_dtype=torch.uint8)\n",
    "m_qat = m_qat.to(device).eval()\n",
    "print(\"QAT ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8398381",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "631dff34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Metrics: size, accuracy, latency/power/energy ---\n",
    "def linear_fp32_size_bytes(mod: torch.nn.Linear):\n",
    "    w_bytes = mod.weight.numel() * 4\n",
    "    b_bytes = (mod.bias.numel() * 4) if (mod.bias is not None) else 0\n",
    "    return w_bytes + b_bytes\n",
    "\n",
    "def linear_int8_size_bytes(out_features, in_features, has_bias=True, per_channel=True):\n",
    "    w_bytes = out_features * in_features * 1  # int8\n",
    "    if per_channel:\n",
    "        scale_bytes = out_features * 4\n",
    "        zp_bytes    = out_features * 4\n",
    "    else:\n",
    "        scale_bytes = 4\n",
    "        zp_bytes    = 4\n",
    "    b_bytes = (out_features * 4) if has_bias else 0\n",
    "    return w_bytes + scale_bytes + zp_bytes + b_bytes\n",
    "\n",
    "def model_size_mb_fp32(model: torch.nn.Module):\n",
    "    total = 0\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, torch.nn.Linear):\n",
    "            total += m.weight.numel() * 4 + (m.bias.numel() * 4 if m.bias is not None else 0)\n",
    "    return total / (1024**2)\n",
    "\n",
    "def model_size_mb_int8_like(model: torch.nn.Module, per_channel=True):\n",
    "    total = 0\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, (torch.nn.Linear, ObservedLinear, FakeQLinear, QuantizedLinear)):\n",
    "            # prefer attributes; fall back to weight.shape\n",
    "            ofe = getattr(m, \"out_features\", None)\n",
    "            ife = getattr(m, \"in_features\", None)\n",
    "            if ofe is None or ife is None:\n",
    "                if hasattr(m, \"weight\"): ofe, ife = m.weight.shape\n",
    "                elif hasattr(m, \"qweight\"): ofe, ife = m.qweight.shape\n",
    "                else: continue\n",
    "            has_bias = getattr(m, \"bias\", None) is not None\n",
    "            total += linear_int8_size_bytes(ofe, ife, has_bias, per_channel=per_channel)\n",
    "    return total / (1024**2)\n",
    "\n",
    "def top1_accuracy(model, loader, device=\"cuda\", dtype=torch.float32):\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "    with torch.inference_mode():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device, dtype=dtype, non_blocking=(device==\"cuda\"))\n",
    "            y = y.to(device)\n",
    "            logits = model(x)\n",
    "            pred = logits.argmax(dim=1)\n",
    "            correct += (pred == y).sum().item()\n",
    "            total += y.numel()\n",
    "    return correct / max(1,total)\n",
    "\n",
    "def latency_power_energy(model, batch_tensor, iters=200, warm=20, device=\"cuda\"):\n",
    "    model.eval()\n",
    "    # warmup\n",
    "    with torch.inference_mode():\n",
    "        for _ in range(warm):\n",
    "            _ = model(batch_tensor)\n",
    "    if device==\"cuda\": torch.cuda.synchronize()\n",
    "\n",
    "    class PowerSampler:\n",
    "        def __init__(self, interval_s=0.02, device_index=0):\n",
    "            self.interval_s = interval_s\n",
    "            self.samples = []\n",
    "            self.device_index = device_index\n",
    "            self._stop = threading.Event()\n",
    "            self._thr = None\n",
    "            try:\n",
    "                import pynvml; self.use_nvml = True\n",
    "            except Exception:\n",
    "                self.use_nvml = False\n",
    "        def _loop_nvml(self):\n",
    "            import pynvml\n",
    "            pynvml.nvmlInit()\n",
    "            h = pynvml.nvmlDeviceGetHandleByIndex(self.device_index)\n",
    "            while not self._stop.is_set():\n",
    "                p_mW = pynvml.nvmlDeviceGetPowerUsage(h)\n",
    "                self.samples.append((time.perf_counter(), p_mW/1000.0))\n",
    "                time.sleep(self.interval_s)\n",
    "            pynvml.nvmlShutdown()\n",
    "        def _loop_smi(self):\n",
    "            while not self._stop.is_set():\n",
    "                try:\n",
    "                    out = subprocess.check_output(\n",
    "                        [\"nvidia-smi\", \"--query-gpu=power.draw\", \"--format=csv,noheader,nounits\", \"-i\", \"0\"],\n",
    "                        stderr=subprocess.DEVNULL\n",
    "                    )\n",
    "                    watts = float(out.decode().strip().splitlines()[0])\n",
    "                except Exception:\n",
    "                    watts = math.nan\n",
    "                self.samples.append((time.perf_counter(), watts))\n",
    "                time.sleep(self.interval_s)\n",
    "        def __enter__(self):\n",
    "            if device==\"cuda\" and torch.cuda.is_available():\n",
    "                target = self._loop_nvml if self.use_nvml else self._loop_smi\n",
    "                self._thr = threading.Thread(target=target, daemon=True); self._thr.start()\n",
    "            return self\n",
    "        def __exit__(self, *args):\n",
    "            if self._thr:\n",
    "                self._stop.set(); self._thr.join()\n",
    "        def summary(self):\n",
    "            if len(self.samples) < 2:\n",
    "                return {\"avg_W\": float(\"nan\"), \"energy_J\": float(\"nan\")}\n",
    "            e = 0.0\n",
    "            finite = [p for _,p in self.samples if not math.isnan(p)]\n",
    "            for (t0,p0),(t1,p1) in zip(self.samples[:-1], self.samples[1:]):\n",
    "                if not (math.isnan(p0) or math.isnan(p1)):\n",
    "                    e += 0.5*(p0+p1)*(t1-t0)\n",
    "            avg_W = sum(finite)/len(finite) if finite else float(\"nan\")\n",
    "            return {\"avg_W\": avg_W, \"energy_J\": e}\n",
    "\n",
    "    with PowerSampler(interval_s=0.02) as ps:\n",
    "        t0 = time.perf_counter()\n",
    "        with torch.inference_mode():\n",
    "            for _ in range(iters):\n",
    "                _ = model(batch_tensor)\n",
    "        if device==\"cuda\": torch.cuda.synchronize()\n",
    "        t1 = time.perf_counter()\n",
    "    lp = ps.summary()\n",
    "    return {\"lat_ms\": (t1-t0)/iters*1000.0, \"avg_W\": lp[\"avg_W\"], \"energy_J\": lp[\"energy_J\"]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb95b5da",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "97443df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                model    lat_ms  avg_W  energy_J    top1   size_MB\n",
      "0       FP32 baseline  0.744754   4.35  0.573032  0.0444  3.048828\n",
      "1  PTQ INT8 (TorchAO)  1.461631   5.25  1.227457  0.0443  0.764221\n",
      "2  QAT INT8 (TorchAO)  1.153280  25.39  4.411358  0.0447  0.764221\n"
     ]
    }
   ],
   "source": [
    "# --- Evaluate FP32 vs PTQ vs QAT ---\n",
    "# fixed batch for latency/power\n",
    "xb_eval, _ = next(iter(val_loader))\n",
    "xb_eval = xb_eval.to(device=device, dtype=torch.float32, non_blocking=(device==\"cuda\"))\n",
    "\n",
    "acc_fp  = top1_accuracy(m_fp_trained, val_loader, device=device, dtype=torch.float32)\n",
    "acc_ptq = top1_accuracy(m_ptq,        val_loader, device=device, dtype=torch.float32)\n",
    "acc_qat = top1_accuracy(m_qat,        val_loader, device=device, dtype=torch.float32)\n",
    "\n",
    "lpe_fp  = latency_power_energy(m_fp_trained, xb_eval, iters=200, warm=20, device=device)\n",
    "lpe_ptq = latency_power_energy(m_ptq,        xb_eval, iters=200, warm=20, device=device)\n",
    "lpe_qat = latency_power_energy(m_qat,        xb_eval, iters=200, warm=20, device=device)\n",
    "\n",
    "size_fp  = model_size_mb_fp32(m_fp_trained)\n",
    "size_ptq = model_size_mb_int8_like(m_ptq, per_channel=True)\n",
    "size_qat = model_size_mb_int8_like(m_qat, per_channel=True)\n",
    "\n",
    "df = pd.DataFrame([\n",
    "    {\"model\":\"FP32 baseline\",\n",
    "     \"lat_ms\": lpe_fp[\"lat_ms\"],  \"avg_W\": lpe_fp[\"avg_W\"],  \"energy_J\": lpe_fp[\"energy_J\"],\n",
    "     \"top1\": acc_fp, \"size_MB\": size_fp},\n",
    "    {\"model\":\"PTQ INT8 (TorchAO)\",\n",
    "     \"lat_ms\": lpe_ptq[\"lat_ms\"], \"avg_W\": lpe_ptq[\"avg_W\"], \"energy_J\": lpe_ptq[\"energy_J\"],\n",
    "     \"top1\": acc_ptq, \"size_MB\": size_ptq},\n",
    "    {\"model\":\"QAT INT8 (TorchAO)\",\n",
    "     \"lat_ms\": lpe_qat[\"lat_ms\"], \"avg_W\": lpe_qat[\"avg_W\"], \"energy_J\": lpe_qat[\"energy_J\"],\n",
    "     \"top1\": acc_qat, \"size_MB\": size_qat},\n",
    "])\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
