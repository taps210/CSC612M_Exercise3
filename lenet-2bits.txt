log/default/ADCprecision=5/batch_size=8/c2cVari=0.003/cellBit=5/d2dVari=0.0/decreasing_lr=200,250/detect=0/grad_scale=1/inference=0/max_level=32/nonlinearityLTD=1.46/nonlinearityLTP=1.75/onoffratio=10/seed=117/subArray=128/t=0/target=0/type=cifar10/v=0/vari=0/wl_activate=8/wl_error=8/wl_grad=5/wl_weight=5
=================FLAGS==================
type: cifar10
batch_size: 8
epochs: 2
grad_scale: 1
seed: 117
log_interval: 100
test_interval: 1
logdir: log/default/ADCprecision=5/batch_size=8/c2cVari=0.003/cellBit=5/d2dVari=0.0/decreasing_lr=200,250/detect=0/grad_scale=1/inference=0/max_level=32/nonlinearityLTD=1.46/nonlinearityLTP=1.75/onoffratio=10/seed=117/subArray=128/t=0/target=0/type=cifar10/v=0/vari=0/wl_activate=8/wl_error=8/wl_grad=5/wl_weight=5
decreasing_lr: 200,250
wl_weight: 5
wl_grad: 5
wl_activate: 8
wl_error: 8
inference: 0
onoffratio: 10
cellBit: 5
subArray: 128
ADCprecision: 5
vari: 0
t: 0
v: 0
detect: 0
target: 0
nonlinearityLTP: 1.75
nonlinearityLTD: 1.46
max_level: 32
d2dVari: 0.0
c2cVari: 0.003
========================================
Building CIFAR-10 data loader with 0 workers
Files already downloaded and verified
Files already downloaded and verified
fan_in     75, float_limit 0.200000, quant limit 1.5, scale 8
fan_in    150, float_limit 0.141421, quant limit 1.5, scale 8
fan_in    400, float_limit 0.086603, quant limit 1.5, scale 16
fan_in    120, float_limit 0.158114, quant limit 1.5, scale 8
fan_in     84, float_limit 0.188982, quant limit 1.5, scale 8
Sequential(
  (0): QConv2d(3, 6, kernel_size=(5, 5), stride=(1, 1), bias=False)
  (1): ReLU()
  (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (3): QConv2d(6, 16, kernel_size=(5, 5), stride=(1, 1), bias=False)
  (4): ReLU()
  (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
)
Sequential(
  (0): QLinear(in_features=400, out_features=120, bias=False)
  (1): ReLU(inplace=True)
  (2): QLinear(in_features=120, out_features=84, bias=False)
  (3): ReLU(inplace=True)
  (4): QLinear(in_features=84, out_features=10, bias=False)
)
decreasing_lr: [200, 250]
training phase
/home/anoobis/csc604m/DNN_NeuroSim_V2.1/Training_pytorch/utee/wage_quantizer.py:17: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/conda/conda-bld/pytorch_1724789115405/work/torch/csrc/tensor/python_tensor.cpp:78.)
  r = torch.cuda.FloatTensor(*x.size()).uniform_()
Train Epoch: 0 [800/50000] Loss: 3.659353 Acc: 0.0000 lr: 1.00e+00
Train Epoch: 0 [1600/50000] Loss: 3.488306 Acc: 0.2500 lr: 1.00e+00
Train Epoch: 0 [2400/50000] Loss: 3.582684 Acc: 0.1250 lr: 1.00e+00
Train Epoch: 0 [3200/50000] Loss: 3.494888 Acc: 0.2500 lr: 1.00e+00
Train Epoch: 0 [4000/50000] Loss: 3.421754 Acc: 0.2500 lr: 1.00e+00
Train Epoch: 0 [4800/50000] Loss: 3.461689 Acc: 0.1250 lr: 1.00e+00
Train Epoch: 0 [5600/50000] Loss: 3.606668 Acc: 0.2500 lr: 1.00e+00
Train Epoch: 0 [6400/50000] Loss: 3.626491 Acc: 0.0000 lr: 1.00e+00
Train Epoch: 0 [7200/50000] Loss: 3.296609 Acc: 0.5000 lr: 1.00e+00
Train Epoch: 0 [8000/50000] Loss: 3.440591 Acc: 0.1250 lr: 1.00e+00
Train Epoch: 0 [8800/50000] Loss: 3.557947 Acc: 0.1250 lr: 1.00e+00
Train Epoch: 0 [9600/50000] Loss: 3.547534 Acc: 0.2500 lr: 1.00e+00
Train Epoch: 0 [10400/50000] Loss: 3.418169 Acc: 0.1250 lr: 1.00e+00
Train Epoch: 0 [11200/50000] Loss: 3.510495 Acc: 0.1250 lr: 1.00e+00
Train Epoch: 0 [12000/50000] Loss: 3.544132 Acc: 0.1250 lr: 1.00e+00
Train Epoch: 0 [12800/50000] Loss: 3.515876 Acc: 0.0000 lr: 1.00e+00
Train Epoch: 0 [13600/50000] Loss: 3.461585 Acc: 0.1250 lr: 1.00e+00
Train Epoch: 0 [14400/50000] Loss: 3.401194 Acc: 0.3750 lr: 1.00e+00
Train Epoch: 0 [15200/50000] Loss: 3.702807 Acc: 0.0000 lr: 1.00e+00
Train Epoch: 0 [16000/50000] Loss: 3.408695 Acc: 0.5000 lr: 1.00e+00
Train Epoch: 0 [16800/50000] Loss: 3.412558 Acc: 0.2500 lr: 1.00e+00
Train Epoch: 0 [17600/50000] Loss: 3.534588 Acc: 0.1250 lr: 1.00e+00
Train Epoch: 0 [18400/50000] Loss: 3.532098 Acc: 0.1250 lr: 1.00e+00
Train Epoch: 0 [19200/50000] Loss: 3.847458 Acc: 0.1250 lr: 1.00e+00
Train Epoch: 0 [20000/50000] Loss: 3.537096 Acc: 0.1250 lr: 1.00e+00
Train Epoch: 0 [20800/50000] Loss: 3.389482 Acc: 0.3750 lr: 1.00e+00
Train Epoch: 0 [21600/50000] Loss: 3.564404 Acc: 0.2500 lr: 1.00e+00
Train Epoch: 0 [22400/50000] Loss: 3.828823 Acc: 0.0000 lr: 1.00e+00
Train Epoch: 0 [23200/50000] Loss: 3.514631 Acc: 0.1250 lr: 1.00e+00
Train Epoch: 0 [24000/50000] Loss: 3.531958 Acc: 0.0000 lr: 1.00e+00
Train Epoch: 0 [24800/50000] Loss: 3.648848 Acc: 0.0000 lr: 1.00e+00
Train Epoch: 0 [25600/50000] Loss: 3.336843 Acc: 0.1250 lr: 1.00e+00
Train Epoch: 0 [26400/50000] Loss: 3.401929 Acc: 0.2500 lr: 1.00e+00
Train Epoch: 0 [27200/50000] Loss: 3.343676 Acc: 0.1250 lr: 1.00e+00
Train Epoch: 0 [28000/50000] Loss: 3.700751 Acc: 0.1250 lr: 1.00e+00
Train Epoch: 0 [28800/50000] Loss: 3.597308 Acc: 0.0000 lr: 1.00e+00
Train Epoch: 0 [29600/50000] Loss: 3.557916 Acc: 0.0000 lr: 1.00e+00
Train Epoch: 0 [30400/50000] Loss: 3.483058 Acc: 0.1250 lr: 1.00e+00
Train Epoch: 0 [31200/50000] Loss: 3.321435 Acc: 0.2500 lr: 1.00e+00
Train Epoch: 0 [32000/50000] Loss: 3.390018 Acc: 0.2500 lr: 1.00e+00
Train Epoch: 0 [32800/50000] Loss: 3.546219 Acc: 0.5000 lr: 1.00e+00
Train Epoch: 0 [33600/50000] Loss: 3.454550 Acc: 0.1250 lr: 1.00e+00
Train Epoch: 0 [34400/50000] Loss: 3.540169 Acc: 0.1250 lr: 1.00e+00
Train Epoch: 0 [35200/50000] Loss: 3.476248 Acc: 0.0000 lr: 1.00e+00
Train Epoch: 0 [36000/50000] Loss: 3.694218 Acc: 0.1250 lr: 1.00e+00
Train Epoch: 0 [36800/50000] Loss: 3.607936 Acc: 0.0000 lr: 1.00e+00
Train Epoch: 0 [37600/50000] Loss: 3.508546 Acc: 0.0000 lr: 1.00e+00
Train Epoch: 0 [38400/50000] Loss: 3.501479 Acc: 0.0000 lr: 1.00e+00
Train Epoch: 0 [39200/50000] Loss: 3.719933 Acc: 0.0000 lr: 1.00e+00
Train Epoch: 0 [40000/50000] Loss: 3.293224 Acc: 0.2500 lr: 1.00e+00
Train Epoch: 0 [40800/50000] Loss: 3.543562 Acc: 0.1250 lr: 1.00e+00
Train Epoch: 0 [41600/50000] Loss: 3.541560 Acc: 0.0000 lr: 1.00e+00
Train Epoch: 0 [42400/50000] Loss: 3.753606 Acc: 0.1250 lr: 1.00e+00
Train Epoch: 0 [43200/50000] Loss: 3.598367 Acc: 0.3750 lr: 1.00e+00
Train Epoch: 0 [44000/50000] Loss: 3.794456 Acc: 0.1250 lr: 1.00e+00
Train Epoch: 0 [44800/50000] Loss: 3.558657 Acc: 0.2500 lr: 1.00e+00
Train Epoch: 0 [45600/50000] Loss: 3.504321 Acc: 0.0000 lr: 1.00e+00
Train Epoch: 0 [46400/50000] Loss: 3.162607 Acc: 0.6250 lr: 1.00e+00
Train Epoch: 0 [47200/50000] Loss: 3.643257 Acc: 0.1250 lr: 1.00e+00
Train Epoch: 0 [48000/50000] Loss: 3.724566 Acc: 0.2500 lr: 1.00e+00
Train Epoch: 0 [48800/50000] Loss: 3.506273 Acc: 0.1250 lr: 1.00e+00
Train Epoch: 0 [49600/50000] Loss: 3.514013 Acc: 0.0000 lr: 1.00e+00
Elapsed 185.69s, 185.69 s/epoch, 0.03 s/batch, ets 185.69s
weight distribution
[-0.05471245 -0.31068614 -0.21702899 -0.25309038  0.20188196  0.43075475
  0.62844729  0.73196393  0.72463745  0.81514484]
delta distribution
[ 7.36111123e-03  1.04166672e-03  2.60416673e-05  3.10019859e-05
 -7.44047647e-05  2.61760168e-02  1.23277344e-02  2.67599151e-03
  1.39170932e-03  3.73657281e-03]
testing phase
	Epoch 0 Test set: Average loss: 3.4771, Accuracy: 1573/10000 (16%)
Saving model to /home/anoobis/csc604m/DNN_NeuroSim_V2.1/Training_pytorch/log/default/ADCprecision=5/batch_size=8/c2cVari=0.003/cellBit=5/d2dVari=0.0/decreasing_lr=200,250/detect=0/grad_scale=1/inference=0/max_level=32/nonlinearityLTD=1.46/nonlinearityLTP=1.75/onoffratio=10/seed=117/subArray=128/t=0/target=0/type=cifar10/v=0/vari=0/wl_activate=8/wl_error=8/wl_grad=5/wl_weight=5/best-0.pth
------------------------------ FloorPlan --------------------------------

Tile and PE size are optimized to maximize memory utilization ( = memory mapped by synapse / total memory on chip)

Desired Conventional Mapped Tile Storage Size: 512x512
Desired Conventional PE Storage Size: 256x256
Desired Novel Mapped Tile Storage Size: 1x256x256
User-defined SubArray Size: 64x64

----------------- # of tile used for each layer -----------------
layer1: 1
layer2: 1
layer3: 1
layer4: 1
layer5: 1

----------------- Speed-up of each layer ------------------
layer1: 32
layer2: 16
layer3: 8
layer4: 2
layer5: 8

----------------- Utilization of each layer ------------------
layer1: 0.164795
layer2: 0.439453
layer3: 0.175781
layer4: 0.922852
layer5: 0.307617
Memory Utilization of Whole Chip: 40.21 % 

---------------------------- FloorPlan Done ------------------------------



-------------------------------------- Hardware Performance --------------------------------------
-------------------- Estimation of Layer 1 ----------------------
layer1's readLatency of Forward is: 3.25516e+09ns
layer1's readDynamicEnergy of Forward is: 1.77413e+10pJ
layer1's readLatency of Activation Gradient is: 5.13048e+06ns
layer1's readDynamicEnergy of Activation Gradient is: 0pJ
layer1's readLatency of Weight Gradient is: 3.71971e+09ns
layer1's readDynamicEnergy of Weight Gradient is: 5.19926e+10pJ
layer1's writeLatency of Weight Update is: 5.25751e+06ns
layer1's writeDynamicEnergy of Weight Update is: 530348pJ
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
layer1's PEAK readLatency of Forward is: 6.26279e+08ns
layer1's PEAK readDynamicEnergy of Forward is: 6.35397e+09pJ
layer1's PEAK readLatency of Activation Gradient is: 5.13048e+06ns
layer1's PEAK readDynamicEnergy of Activation Gradient is: 0pJ
layer1's PEAK readLatency of Weight Gradient is: 3.14556e+09ns
layer1's PEAK readDynamicEnergy of Weight Gradient is: 4.58143e+10pJ
layer1's PEAK writeLatency of Weight Update is: 5.25205e+06ns
layer1's PEAK writeDynamicEnergy of Weight Update is: 513375pJ
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
layer1's leakagePower is: 2.24016uW
layer1's leakageEnergy is: 2.92143e+07pJ

************************ Breakdown of Latency and Dynamic Energy *************************

----------- ADC (or S/As and precharger for SRAM) readLatency is : 5.488e+08ns
----------- Accumulation Circuits (subarray level: adders, shiftAdds; PE/Tile/Global level: accumulation units) readLatency is : 3.18452e+07ns
----------- Synaptic Array w/o ADC (Forward + Activate Gradient) readLatency is : 4.56338e+07ns
----------- Buffer buffer latency is: 2.86642e+09ns
----------- Interconnect latency is: 3.40818e+08ns
----------- Weight Gradient Calculation readLatency is : 3.14556e+09ns
----------- Weight Update writeLatency is : 5.25205e+06ns
----------- DRAM data transfer Latency is : 694105ns
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
----------- ADC (or S/As and precharger for SRAM) readDynamicEnergy is : 1.7436e+09pJ
----------- Accumulation Circuits (subarray level: adders, shiftAdds; PE/Tile/Global level: accumulation units) readDynamicEnergy is : 3.53674e+09pJ
----------- Synaptic Array w/o ADC (Forward + Activate Gradient) readDynamicEnergy is : 1.07363e+09pJ
----------- Buffer readDynamicEnergy is: 5.41622e+08pJ
----------- Interconnect readDynamicEnergy is: 7.82969e+09pJ
----------- Weight Gradient Calculation readDynamicEnergy is : 4.58143e+10pJ
----------- Weight Update writeDynamicEnergy is : 513375pJ
----------- DRAM data transfer Energy is : 4.11466e+10pJ

************************ Breakdown of Latency and Dynamic Energy *************************

-------------------- Estimation of Layer 2 ----------------------
layer2's readLatency of Forward is: 8.47751e+08ns
layer2's readDynamicEnergy of Forward is: 4.26281e+09pJ
layer2's readLatency of Activation Gradient is: 8.51158e+08ns
layer2's readDynamicEnergy of Activation Gradient is: 4.26384e+09pJ
layer2's readLatency of Weight Gradient is: 1.58635e+09ns
layer2's readDynamicEnergy of Weight Gradient is: 3.6918e+10pJ
layer2's writeLatency of Weight Update is: 9.78219e+06ns
layer2's writeDynamicEnergy of Weight Update is: 570494pJ
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
layer2's PEAK readLatency of Forward is: 1.61364e+08ns
layer2's PEAK readDynamicEnergy of Forward is: 1.50501e+09pJ
layer2's PEAK readLatency of Activation Gradient is: 1.64771e+08ns
layer2's PEAK readDynamicEnergy of Activation Gradient is: 1.50604e+09pJ
layer2's PEAK readLatency of Weight Gradient is: 1.4213e+09ns
layer2's PEAK readDynamicEnergy of Weight Gradient is: 2.12595e+10pJ
layer2's PEAK writeLatency of Weight Update is: 9.75308e+06ns
layer2's PEAK writeDynamicEnergy of Weight Update is: 479971pJ
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
layer2's leakagePower is: 3.83889uW
layer2's leakageEnergy is: 2.60877e+07pJ

************************ Breakdown of Latency and Dynamic Energy *************************

----------- ADC (or S/As and precharger for SRAM) readLatency is : 2.8e+08ns
----------- Accumulation Circuits (subarray level: adders, shiftAdds; PE/Tile/Global level: accumulation units) readLatency is : 1.63212e+07ns
----------- Synaptic Array w/o ADC (Forward + Activate Gradient) readLatency is : 2.71228e+07ns
----------- Buffer buffer latency is: 1.38108e+09ns
----------- Interconnect latency is: 1.79757e+08ns
----------- Weight Gradient Calculation readLatency is : 1.4213e+09ns
----------- Weight Update writeLatency is : 9.75308e+06ns
----------- DRAM data transfer Latency is : 438316ns
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
----------- ADC (or S/As and precharger for SRAM) readDynamicEnergy is : 1.29702e+09pJ
----------- Accumulation Circuits (subarray level: adders, shiftAdds; PE/Tile/Global level: accumulation units) readDynamicEnergy is : 1.35334e+09pJ
----------- Synaptic Array w/o ADC (Forward + Activate Gradient) readDynamicEnergy is : 3.60693e+08pJ
----------- Buffer readDynamicEnergy is: 2.92931e+08pJ
----------- Interconnect readDynamicEnergy is: 4.08873e+09pJ
----------- Weight Gradient Calculation readDynamicEnergy is : 2.12595e+10pJ
----------- Weight Update writeDynamicEnergy is : 479971pJ
----------- DRAM data transfer Energy is : 2.59834e+10pJ

************************ Breakdown of Latency and Dynamic Energy *************************

-------------------- Estimation of Layer 3 ----------------------
layer3's readLatency of Forward is: 2.82164e+08ns
layer3's readDynamicEnergy of Forward is: 1.20178e+09pJ
layer3's readLatency of Activation Gradient is: 2.83048e+08ns
layer3's readDynamicEnergy of Activation Gradient is: 1.15237e+09pJ
layer3's readLatency of Weight Gradient is: 2.55326e+08ns
layer3's readDynamicEnergy of Weight Gradient is: 1.46194e+10pJ
layer3's writeLatency of Weight Update is: 23288.1ns
layer3's writeDynamicEnergy of Weight Update is: 74509pJ
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
layer3's PEAK readLatency of Forward is: 8.01563e+07ns
layer3's PEAK readDynamicEnergy of Forward is: 5.57096e+08pJ
layer3's PEAK readLatency of Activation Gradient is: 8.104e+07ns
layer3's PEAK readDynamicEnergy of Activation Gradient is: 5.07682e+08pJ
layer3's PEAK readLatency of Weight Gradient is: 2.0816e+08ns
layer3's PEAK readDynamicEnergy of Weight Gradient is: 2.5291e+09pJ
layer3's PEAK writeLatency of Weight Update is: 0ns
layer3's PEAK writeDynamicEnergy of Weight Update is: 2090.6pJ
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
layer3's leakagePower is: 3.66667uW
layer3's leakageEnergy is: 8.28979e+06pJ

************************ Breakdown of Latency and Dynamic Energy *************************

----------- ADC (or S/As and precharger for SRAM) readLatency is : 1.4e+08ns
----------- Accumulation Circuits (subarray level: adders, shiftAdds; PE/Tile/Global level: accumulation units) readLatency is : 6.81498e+06ns
----------- Synaptic Array w/o ADC (Forward + Activate Gradient) readLatency is : 1.43813e+07ns
----------- Buffer buffer latency is: 4.22737e+08ns
----------- Interconnect latency is: 4.68699e+07ns
----------- Weight Gradient Calculation readLatency is : 2.0816e+08ns
----------- Weight Update writeLatency is : 0ns
----------- DRAM data transfer Latency is : 225789ns
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
----------- ADC (or S/As and precharger for SRAM) readDynamicEnergy is : 2.74887e+08pJ
----------- Accumulation Circuits (subarray level: adders, shiftAdds; PE/Tile/Global level: accumulation units) readDynamicEnergy is : 5.91429e+08pJ
----------- Synaptic Array w/o ADC (Forward + Activate Gradient) readDynamicEnergy is : 1.98461e+08pJ
----------- Buffer readDynamicEnergy is: 1.55479e+08pJ
----------- Interconnect readDynamicEnergy is: 1.02385e+09pJ
----------- Weight Gradient Calculation readDynamicEnergy is : 2.5291e+09pJ
----------- Weight Update writeDynamicEnergy is : 2090.6pJ
----------- DRAM data transfer Energy is : 1.33848e+10pJ

************************ Breakdown of Latency and Dynamic Energy *************************

-------------------- Estimation of Layer 4 ----------------------
layer4's readLatency of Forward is: 2.37224e+07ns
layer4's readDynamicEnergy of Forward is: 8.53716e+07pJ
layer4's readLatency of Activation Gradient is: 2.40018e+07ns
layer4's readDynamicEnergy of Activation Gradient is: 8.65956e+07pJ
layer4's readLatency of Weight Gradient is: 8.39256e+07ns
layer4's readDynamicEnergy of Weight Gradient is: 6.36059e+10pJ
layer4's writeLatency of Weight Update is: 59118.6ns
layer4's writeDynamicEnergy of Weight Update is: 48187.6pJ
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
layer4's PEAK readLatency of Forward is: 1.27917e+07ns
layer4's PEAK readDynamicEnergy of Forward is: 4.09189e+07pJ
layer4's PEAK readLatency of Activation Gradient is: 1.30711e+07ns
layer4's PEAK readDynamicEnergy of Activation Gradient is: 4.21429e+07pJ
layer4's PEAK readLatency of Weight Gradient is: 4.1989e+06ns
layer4's PEAK readDynamicEnergy of Weight Gradient is: 5.88484e+08pJ
layer4's PEAK writeLatency of Weight Update is: 0ns
layer4's PEAK writeDynamicEnergy of Weight Update is: 2787.46pJ
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
layer4's leakagePower is: 0.979497uW
layer4's leakageEnergy is: 186983pJ

************************ Breakdown of Latency and Dynamic Energy *************************

----------- ADC (or S/As and precharger for SRAM) readLatency is : 2.24e+07ns
----------- Accumulation Circuits (subarray level: adders, shiftAdds; PE/Tile/Global level: accumulation units) readLatency is : 1.2998e+06ns
----------- Synaptic Array w/o ADC (Forward + Activate Gradient) readLatency is : 2.16301e+06ns
----------- Buffer buffer latency is: 1.46573e+08ns
----------- Interconnect latency is: 1.24812e+06ns
----------- Weight Gradient Calculation readLatency is : 4.1989e+06ns
----------- Weight Update writeLatency is : 0ns
----------- DRAM data transfer Latency is : 1.062e+06ns
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
----------- ADC (or S/As and precharger for SRAM) readDynamicEnergy is : 4.17121e+07pJ
----------- Accumulation Circuits (subarray level: adders, shiftAdds; PE/Tile/Global level: accumulation units) readDynamicEnergy is : 3.21112e+07pJ
----------- Synaptic Array w/o ADC (Forward + Activate Gradient) readDynamicEnergy is : 9.23858e+06pJ
----------- Buffer readDynamicEnergy is: 1.76234e+08pJ
----------- Interconnect readDynamicEnergy is: 3.00567e+07pJ
----------- Weight Gradient Calculation readDynamicEnergy is : 5.88484e+08pJ
----------- Weight Update writeDynamicEnergy is : 2787.46pJ
----------- DRAM data transfer Energy is : 6.29554e+10pJ

************************ Breakdown of Latency and Dynamic Energy *************************

-------------------- Estimation of Layer 5 ----------------------
layer5's readLatency of Forward is: 6.19007e+06ns
layer5's readDynamicEnergy of Forward is: 2.37112e+07pJ
layer5's readLatency of Activation Gradient is: 6.24742e+06ns
layer5's readDynamicEnergy of Activation Gradient is: 2.40619e+07pJ
layer5's readLatency of Weight Gradient is: 9.53493e+06ns
layer5's readDynamicEnergy of Weight Gradient is: 5.29526e+09pJ
layer5's writeLatency of Weight Update is: 4926.55ns
layer5's writeDynamicEnergy of Weight Update is: 9125.98pJ
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
layer5's PEAK readLatency of Forward is: 3.21043e+06ns
layer5's PEAK readDynamicEnergy of Forward is: 6.55859e+06pJ
layer5's PEAK readLatency of Activation Gradient is: 3.26778e+06ns
layer5's PEAK readDynamicEnergy of Activation Gradient is: 6.90927e+06pJ
layer5's PEAK readLatency of Weight Gradient is: 2.15838e+06ns
layer5's PEAK readDynamicEnergy of Weight Gradient is: 4.07341e+07pJ
layer5's PEAK writeLatency of Weight Update is: 0ns
layer5's PEAK writeDynamicEnergy of Weight Update is: 696.866pJ
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
layer5's leakagePower is: 0.979497uW
layer5's leakageEnergy is: 48730pJ

************************ Breakdown of Latency and Dynamic Energy *************************

----------- ADC (or S/As and precharger for SRAM) readLatency is : 5.6e+06ns
----------- Accumulation Circuits (subarray level: adders, shiftAdds; PE/Tile/Global level: accumulation units) readLatency is : 324951ns
----------- Synaptic Array w/o ADC (Forward + Activate Gradient) readLatency is : 553253ns
----------- Buffer buffer latency is: 1.681e+07ns
----------- Interconnect latency is: 378491ns
----------- Weight Gradient Calculation readLatency is : 2.15838e+06ns
----------- Weight Update writeLatency is : 0ns
----------- DRAM data transfer Latency is : 89368.4ns
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
----------- ADC (or S/As and precharger for SRAM) readDynamicEnergy is : 2.69662e+06pJ
----------- Accumulation Circuits (subarray level: adders, shiftAdds; PE/Tile/Global level: accumulation units) readDynamicEnergy is : 8.45401e+06pJ
----------- Synaptic Array w/o ADC (Forward + Activate Gradient) readDynamicEnergy is : 2.31723e+06pJ
----------- Buffer readDynamicEnergy is: 2.36278e+07pJ
----------- Interconnect readDynamicEnergy is: 1.49583e+07pJ
----------- Weight Gradient Calculation readDynamicEnergy is : 4.07341e+07pJ
----------- Weight Update writeDynamicEnergy is : 696.866pJ
----------- DRAM data transfer Energy is : 5.29776e+09pJ

************************ Breakdown of Latency and Dynamic Energy *************************

------------------------------ Summary --------------------------------

ChipArea : 4.86416e+06um^2
Chip total CIM (Forward+Activation Gradient) array : 15032.4um^2
Total IC Area on chip (Global and Tile/PE local): 910823um^2
Total ADC (or S/As and precharger for SRAM) Area on chip : 1.084e+06um^2
Total Accumulation Circuits (subarray level: adders, shiftAdds; PE/Tile/Global level: accumulation units) on chip : 861296um^2
Other Peripheries (e.g. decoders, mux, switchmatrix, buffers, pooling and activation units) : 645892um^2
Weight Gradient Calculation : 1.34711e+06um^2

-----------------------------------Chip layer-by-layer Estimation---------------------------------
Chip readLatency of Forward (per epoch) is: 4.41498e+09ns
Chip readDynamicEnergy of Forward (per epoch) is: 2.3315e+10pJ
Chip readLatency of Activation Gradient (per epoch) is: 1.16959e+09ns
Chip readDynamicEnergy of Activation Gradient (per epoch) is: 5.52687e+09pJ
Chip readLatency of Weight Gradient (per epoch) is: 5.65485e+09ns
Chip readDynamicEnergy of Weight Gradient (per epoch) is: 1.72431e+11pJ
Chip writeLatency of Weight Update (per epoch) is: 1.5127e+07ns
Chip writeDynamicEnergy of Weight Update (per epoch) is: 1.23266e+06pJ
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Chip total Latency (per epoch) is: 1.12545e+10ns
Chip total Energy (per epoch) is: 2.01274e+11pJ
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Chip PEAK readLatency of Forward (per epoch) is: 8.83801e+08ns
Chip PEAK readDynamicEnergy of Forward (per epoch) is: 8.46355e+09pJ
Chip PEAK readLatency of Activation Gradient (per epoch) is: 2.67281e+08ns
Chip PEAK readDynamicEnergy of Activation Gradient (per epoch) is: 2.06278e+09pJ
Chip PEAK readLatency of Weight Gradient (per epoch) is: 4.78137e+09ns
Chip PEAK readDynamicEnergy of Weight Gradient (per epoch) is: 7.02321e+10pJ
Chip PEAK writeLatency of Weight Update (per epoch) is: 1.50051e+07ns
Chip PEAK writeDynamicEnergy of Weight Update (per epoch) is: 998921pJ
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Chip PEAK total Latency (per epoch) is: 5.94746e+09ns
Chip PEAK total Energy (per epoch) is: 8.07594e+10pJ
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Chip leakage Energy is: 6.38275e+07pJ
Chip leakage Power is: 11.7047uW

************************ Breakdown of Latency and Dynamic Energy *************************

----------- ADC (or S/As and precharger for SRAM) readLatency is : 9.968e+08ns
----------- Accumulation Circuits (subarray level: adders, shiftAdds; PE/Tile/Global level: accumulation units) readLatency is : 5.66061e+07ns
----------- Synaptic Array w/o ADC (Forward + Activate Gradient) readLatency is : 8.98542e+07ns
----------- Buffer readLatency is: 4.83362e+09ns
----------- Interconnect readLatency is: 5.69071e+08ns
----------- Weight Gradient Calculation readLatency is : 4.78137e+09ns
----------- Weight Update writeLatency is : 1.50051e+07ns
----------- DRAM data transfer Latency is : 2.50958e+06ns
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
----------- ADC (or S/As and precharger for SRAM) readDynamicEnergy is : 3.35991e+09pJ
----------- Accumulation Circuits (subarray level: adders, shiftAdds; PE/Tile/Global level: accumulation units) readDynamicEnergy is : 5.52208e+09pJ
----------- Synaptic Array w/o ADC (Forward + Activate Gradient) readDynamicEnergy is : 1.64434e+09pJ
----------- Buffer readDynamicEnergy is: 1.18989e+09pJ
----------- Interconnect readDynamicEnergy is: 1.29873e+10pJ
----------- Weight Gradient Calculation readDynamicEnergy is : 7.02321e+10pJ
----------- Weight Update writeDynamicEnergy is : 998921pJ
----------- DRAM data transfer DynamicEnergy is : 1.48768e+11pJ

************************ Breakdown of Latency and Dynamic Energy *************************


-----------------------------------Chip layer-by-layer Performance---------------------------------
Energy Efficiency TOPS/W: 1.24644
Throughput TOPS: 0.0222982
Throughput FPS: 0.088853
--------------------------------------------------------------------------
Peak Energy Efficiency TOPS/W: 3.10745
Peak Throughput TOPS: 0.0421955
Peak Throughput FPS: 0.168139
-------------------------------------- Hardware Performance Done --------------------------------------

------------------------------ Simulation Performance --------------------------------
Total Run-time of NeuroSim: 0 seconds
------------------------------ Simulation Performance --------------------------------
training phase
Train Epoch: 1 [800/50000] Loss: 3.402027 Acc: 0.1250 lr: 1.00e+00
Train Epoch: 1 [1600/50000] Loss: 3.408064 Acc: 0.1250 lr: 1.00e+00
Train Epoch: 1 [2400/50000] Loss: 3.363067 Acc: 0.2500 lr: 1.00e+00
Train Epoch: 1 [3200/50000] Loss: 3.204823 Acc: 0.3750 lr: 1.00e+00
Train Epoch: 1 [4000/50000] Loss: 3.487660 Acc: 0.1250 lr: 1.00e+00
Train Epoch: 1 [4800/50000] Loss: 3.143798 Acc: 0.5000 lr: 1.00e+00
Train Epoch: 1 [5600/50000] Loss: 3.686157 Acc: 0.2500 lr: 1.00e+00
Train Epoch: 1 [6400/50000] Loss: 3.638776 Acc: 0.0000 lr: 1.00e+00
Train Epoch: 1 [7200/50000] Loss: 3.678415 Acc: 0.0000 lr: 1.00e+00
Train Epoch: 1 [8000/50000] Loss: 3.357390 Acc: 0.2500 lr: 1.00e+00
Train Epoch: 1 [8800/50000] Loss: 3.620543 Acc: 0.1250 lr: 1.00e+00
Train Epoch: 1 [9600/50000] Loss: 3.411593 Acc: 0.2500 lr: 1.00e+00
Train Epoch: 1 [10400/50000] Loss: 3.837119 Acc: 0.0000 lr: 1.00e+00
Train Epoch: 1 [11200/50000] Loss: 3.525424 Acc: 0.1250 lr: 1.00e+00
Train Epoch: 1 [12000/50000] Loss: 3.608920 Acc: 0.3750 lr: 1.00e+00
Train Epoch: 1 [12800/50000] Loss: 3.332823 Acc: 0.2500 lr: 1.00e+00
Train Epoch: 1 [13600/50000] Loss: 3.366760 Acc: 0.2500 lr: 1.00e+00
Train Epoch: 1 [14400/50000] Loss: 3.275543 Acc: 0.2500 lr: 1.00e+00
Train Epoch: 1 [15200/50000] Loss: 3.579147 Acc: 0.0000 lr: 1.00e+00
Train Epoch: 1 [16000/50000] Loss: 3.502574 Acc: 0.2500 lr: 1.00e+00
Train Epoch: 1 [16800/50000] Loss: 3.500282 Acc: 0.0000 lr: 1.00e+00
Train Epoch: 1 [17600/50000] Loss: 3.398948 Acc: 0.2500 lr: 1.00e+00
Train Epoch: 1 [18400/50000] Loss: 3.574751 Acc: 0.1250 lr: 1.00e+00
Train Epoch: 1 [19200/50000] Loss: 3.391541 Acc: 0.5000 lr: 1.00e+00
Train Epoch: 1 [20000/50000] Loss: 3.454568 Acc: 0.0000 lr: 1.00e+00
Train Epoch: 1 [20800/50000] Loss: 3.307403 Acc: 0.5000 lr: 1.00e+00
Train Epoch: 1 [21600/50000] Loss: 3.453345 Acc: 0.1250 lr: 1.00e+00
Train Epoch: 1 [22400/50000] Loss: 3.493499 Acc: 0.2500 lr: 1.00e+00
Train Epoch: 1 [23200/50000] Loss: 3.643480 Acc: 0.1250 lr: 1.00e+00
Train Epoch: 1 [24000/50000] Loss: 3.518039 Acc: 0.2500 lr: 1.00e+00
Train Epoch: 1 [24800/50000] Loss: 3.532765 Acc: 0.2500 lr: 1.00e+00
Train Epoch: 1 [25600/50000] Loss: 3.479645 Acc: 0.1250 lr: 1.00e+00
Train Epoch: 1 [26400/50000] Loss: 3.756609 Acc: 0.1250 lr: 1.00e+00
Train Epoch: 1 [27200/50000] Loss: 3.444291 Acc: 0.1250 lr: 1.00e+00
Train Epoch: 1 [28000/50000] Loss: 3.297948 Acc: 0.1250 lr: 1.00e+00
Train Epoch: 1 [28800/50000] Loss: 3.511435 Acc: 0.2500 lr: 1.00e+00
Train Epoch: 1 [29600/50000] Loss: 3.498959 Acc: 0.1250 lr: 1.00e+00
Train Epoch: 1 [30400/50000] Loss: 3.346054 Acc: 0.2500 lr: 1.00e+00
Train Epoch: 1 [31200/50000] Loss: 3.303454 Acc: 0.3750 lr: 1.00e+00
Train Epoch: 1 [32000/50000] Loss: 3.629858 Acc: 0.0000 lr: 1.00e+00
Train Epoch: 1 [32800/50000] Loss: 3.781051 Acc: 0.0000 lr: 1.00e+00
Train Epoch: 1 [33600/50000] Loss: 3.795366 Acc: 0.0000 lr: 1.00e+00
Train Epoch: 1 [34400/50000] Loss: 3.766426 Acc: 0.1250 lr: 1.00e+00
Train Epoch: 1 [35200/50000] Loss: 3.628546 Acc: 0.0000 lr: 1.00e+00
Train Epoch: 1 [36000/50000] Loss: 4.015483 Acc: 0.2500 lr: 1.00e+00
Train Epoch: 1 [36800/50000] Loss: 3.438594 Acc: 0.1250 lr: 1.00e+00
Train Epoch: 1 [37600/50000] Loss: 3.429815 Acc: 0.1250 lr: 1.00e+00
Train Epoch: 1 [38400/50000] Loss: 3.600137 Acc: 0.1250 lr: 1.00e+00
Train Epoch: 1 [39200/50000] Loss: 3.485166 Acc: 0.2500 lr: 1.00e+00
Train Epoch: 1 [40000/50000] Loss: 3.237397 Acc: 0.5000 lr: 1.00e+00
Train Epoch: 1 [40800/50000] Loss: 3.256300 Acc: 0.2500 lr: 1.00e+00
Train Epoch: 1 [41600/50000] Loss: 3.340854 Acc: 0.2500 lr: 1.00e+00
Train Epoch: 1 [42400/50000] Loss: 3.753903 Acc: 0.1250 lr: 1.00e+00
Train Epoch: 1 [43200/50000] Loss: 3.556803 Acc: 0.2500 lr: 1.00e+00
Train Epoch: 1 [44000/50000] Loss: 3.557115 Acc: 0.2500 lr: 1.00e+00
Train Epoch: 1 [44800/50000] Loss: 3.814550 Acc: 0.1250 lr: 1.00e+00
Train Epoch: 1 [45600/50000] Loss: 3.523380 Acc: 0.1250 lr: 1.00e+00
Train Epoch: 1 [46400/50000] Loss: 3.753219 Acc: 0.1250 lr: 1.00e+00
Train Epoch: 1 [47200/50000] Loss: 3.414745 Acc: 0.2500 lr: 1.00e+00
Train Epoch: 1 [48000/50000] Loss: 3.227395 Acc: 0.3750 lr: 1.00e+00
Train Epoch: 1 [48800/50000] Loss: 3.479671 Acc: 0.2500 lr: 1.00e+00
Train Epoch: 1 [49600/50000] Loss: 3.515237 Acc: 0.1250 lr: 1.00e+00
Elapsed 363.45s, 181.72 s/epoch, 0.03 s/batch, ets 0.00s
weight distribution
[-0.1187704  -0.35607198 -0.23247853 -0.26531857  0.18817349  0.47395566
  0.58376259  0.72811276  0.72142178  0.80970269]
delta distribution
[-9.72222246e-04  5.98958344e-04 -2.34375002e-05 -2.48015876e-05
  0.00000000e+00  2.68541146e-02  1.18870391e-02  3.42322164e-03
  1.24484452e-03  3.05150473e-03]
testing phase
	Epoch 1 Test set: Average loss: 3.4873, Accuracy: 1716/10000 (17%)
Removing old model /home/anoobis/csc604m/DNN_NeuroSim_V2.1/Training_pytorch/log/default/ADCprecision=5/batch_size=8/c2cVari=0.003/cellBit=5/d2dVari=0.0/decreasing_lr=200,250/detect=0/grad_scale=1/inference=0/max_level=32/nonlinearityLTD=1.46/nonlinearityLTP=1.75/onoffratio=10/seed=117/subArray=128/t=0/target=0/type=cifar10/v=0/vari=0/wl_activate=8/wl_error=8/wl_grad=5/wl_weight=5/best-0.pth
Saving model to /home/anoobis/csc604m/DNN_NeuroSim_V2.1/Training_pytorch/log/default/ADCprecision=5/batch_size=8/c2cVari=0.003/cellBit=5/d2dVari=0.0/decreasing_lr=200,250/detect=0/grad_scale=1/inference=0/max_level=32/nonlinearityLTD=1.46/nonlinearityLTP=1.75/onoffratio=10/seed=117/subArray=128/t=0/target=0/type=cifar10/v=0/vari=0/wl_activate=8/wl_error=8/wl_grad=5/wl_weight=5/best-1.pth
------------------------------ FloorPlan --------------------------------

Tile and PE size are optimized to maximize memory utilization ( = memory mapped by synapse / total memory on chip)

Desired Conventional Mapped Tile Storage Size: 512x512
Desired Conventional PE Storage Size: 256x256
Desired Novel Mapped Tile Storage Size: 1x256x256
User-defined SubArray Size: 64x64

----------------- # of tile used for each layer -----------------
layer1: 1
layer2: 1
layer3: 1
layer4: 1
layer5: 1

----------------- Speed-up of each layer ------------------
layer1: 32
layer2: 16
layer3: 8
layer4: 2
layer5: 8

----------------- Utilization of each layer ------------------
layer1: 0.164795
layer2: 0.439453
layer3: 0.175781
layer4: 0.922852
layer5: 0.307617
Memory Utilization of Whole Chip: 40.21 % 

---------------------------- FloorPlan Done ------------------------------



-------------------------------------- Hardware Performance --------------------------------------
-------------------- Estimation of Layer 1 ----------------------
layer1's readLatency of Forward is: 3.25516e+09ns
layer1's readDynamicEnergy of Forward is: 1.77277e+10pJ
layer1's readLatency of Activation Gradient is: 5.13048e+06ns
layer1's readDynamicEnergy of Activation Gradient is: 0pJ
layer1's readLatency of Weight Gradient is: 3.71971e+09ns
layer1's readDynamicEnergy of Weight Gradient is: 5.19926e+10pJ
layer1's writeLatency of Weight Update is: 3.75648e+06ns
layer1's writeDynamicEnergy of Weight Update is: 365970pJ
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
layer1's PEAK readLatency of Forward is: 6.26279e+08ns
layer1's PEAK readDynamicEnergy of Forward is: 6.34039e+09pJ
layer1's PEAK readLatency of Activation Gradient is: 5.13048e+06ns
layer1's PEAK readDynamicEnergy of Activation Gradient is: 0pJ
layer1's PEAK readLatency of Weight Gradient is: 3.14556e+09ns
layer1's PEAK readDynamicEnergy of Weight Gradient is: 4.58143e+10pJ
layer1's PEAK writeLatency of Weight Update is: 3.75102e+06ns
layer1's PEAK writeDynamicEnergy of Weight Update is: 348997pJ
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
layer1's leakagePower is: 2.24016uW
layer1's leakageEnergy is: 2.92143e+07pJ

************************ Breakdown of Latency and Dynamic Energy *************************

----------- ADC (or S/As and precharger for SRAM) readLatency is : 5.488e+08ns
----------- Accumulation Circuits (subarray level: adders, shiftAdds; PE/Tile/Global level: accumulation units) readLatency is : 3.18452e+07ns
----------- Synaptic Array w/o ADC (Forward + Activate Gradient) readLatency is : 4.56338e+07ns
----------- Buffer buffer latency is: 2.86642e+09ns
----------- Interconnect latency is: 3.40818e+08ns
----------- Weight Gradient Calculation readLatency is : 3.14556e+09ns
----------- Weight Update writeLatency is : 3.75102e+06ns
----------- DRAM data transfer Latency is : 694105ns
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
----------- ADC (or S/As and precharger for SRAM) readDynamicEnergy is : 1.73002e+09pJ
----------- Accumulation Circuits (subarray level: adders, shiftAdds; PE/Tile/Global level: accumulation units) readDynamicEnergy is : 3.53674e+09pJ
----------- Synaptic Array w/o ADC (Forward + Activate Gradient) readDynamicEnergy is : 1.07363e+09pJ
----------- Buffer readDynamicEnergy is: 5.41622e+08pJ
----------- Interconnect readDynamicEnergy is: 7.82969e+09pJ
----------- Weight Gradient Calculation readDynamicEnergy is : 4.58143e+10pJ
----------- Weight Update writeDynamicEnergy is : 348997pJ
----------- DRAM data transfer Energy is : 4.11466e+10pJ

************************ Breakdown of Latency and Dynamic Energy *************************

-------------------- Estimation of Layer 2 ----------------------
layer2's readLatency of Forward is: 8.47751e+08ns
layer2's readDynamicEnergy of Forward is: 4.27613e+09pJ
layer2's readLatency of Activation Gradient is: 8.51158e+08ns
layer2's readDynamicEnergy of Activation Gradient is: 4.26558e+09pJ
layer2's readLatency of Weight Gradient is: 1.58783e+09ns
layer2's readDynamicEnergy of Weight Gradient is: 3.99849e+10pJ
layer2's writeLatency of Weight Update is: 4.53114e+06ns
layer2's writeDynamicEnergy of Weight Update is: 321181pJ
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
layer2's PEAK readLatency of Forward is: 1.61364e+08ns
layer2's PEAK readDynamicEnergy of Forward is: 1.51834e+09pJ
layer2's PEAK readLatency of Activation Gradient is: 1.64771e+08ns
layer2's PEAK readDynamicEnergy of Activation Gradient is: 1.50779e+09pJ
layer2's PEAK readLatency of Weight Gradient is: 1.42277e+09ns
layer2's PEAK readDynamicEnergy of Weight Gradient is: 2.43263e+10pJ
layer2's PEAK writeLatency of Weight Update is: 4.50203e+06ns
layer2's PEAK writeDynamicEnergy of Weight Update is: 230658pJ
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
layer2's leakagePower is: 3.83889uW
layer2's leakageEnergy is: 2.60877e+07pJ

************************ Breakdown of Latency and Dynamic Energy *************************

----------- ADC (or S/As and precharger for SRAM) readLatency is : 2.8e+08ns
----------- Accumulation Circuits (subarray level: adders, shiftAdds; PE/Tile/Global level: accumulation units) readLatency is : 1.63212e+07ns
----------- Synaptic Array w/o ADC (Forward + Activate Gradient) readLatency is : 2.71228e+07ns
----------- Buffer buffer latency is: 1.38108e+09ns
----------- Interconnect latency is: 1.79757e+08ns
----------- Weight Gradient Calculation readLatency is : 1.42277e+09ns
----------- Weight Update writeLatency is : 4.50203e+06ns
----------- DRAM data transfer Latency is : 438316ns
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
----------- ADC (or S/As and precharger for SRAM) readDynamicEnergy is : 1.30812e+09pJ
----------- Accumulation Circuits (subarray level: adders, shiftAdds; PE/Tile/Global level: accumulation units) readDynamicEnergy is : 1.35334e+09pJ
----------- Synaptic Array w/o ADC (Forward + Activate Gradient) readDynamicEnergy is : 3.64663e+08pJ
----------- Buffer readDynamicEnergy is: 2.92931e+08pJ
----------- Interconnect readDynamicEnergy is: 4.08873e+09pJ
----------- Weight Gradient Calculation readDynamicEnergy is : 2.43263e+10pJ
----------- Weight Update writeDynamicEnergy is : 230658pJ
----------- DRAM data transfer Energy is : 2.59834e+10pJ

************************ Breakdown of Latency and Dynamic Energy *************************

-------------------- Estimation of Layer 3 ----------------------
layer3's readLatency of Forward is: 2.82164e+08ns
layer3's readDynamicEnergy of Forward is: 1.20178e+09pJ
layer3's readLatency of Activation Gradient is: 2.83048e+08ns
layer3's readDynamicEnergy of Activation Gradient is: 1.15237e+09pJ
layer3's readLatency of Weight Gradient is: 2.55322e+08ns
layer3's readDynamicEnergy of Weight Gradient is: 1.46193e+10pJ
layer3's writeLatency of Weight Update is: 23288.1ns
layer3's writeDynamicEnergy of Weight Update is: 74509pJ
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
layer3's PEAK readLatency of Forward is: 8.01563e+07ns
layer3's PEAK readDynamicEnergy of Forward is: 5.57096e+08pJ
layer3's PEAK readLatency of Activation Gradient is: 8.104e+07ns
layer3's PEAK readDynamicEnergy of Activation Gradient is: 5.07682e+08pJ
layer3's PEAK readLatency of Weight Gradient is: 2.08155e+08ns
layer3's PEAK readDynamicEnergy of Weight Gradient is: 2.52905e+09pJ
layer3's PEAK writeLatency of Weight Update is: 0ns
layer3's PEAK writeDynamicEnergy of Weight Update is: 2090.6pJ
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
layer3's leakagePower is: 3.66667uW
layer3's leakageEnergy is: 8.28979e+06pJ

************************ Breakdown of Latency and Dynamic Energy *************************

----------- ADC (or S/As and precharger for SRAM) readLatency is : 1.4e+08ns
----------- Accumulation Circuits (subarray level: adders, shiftAdds; PE/Tile/Global level: accumulation units) readLatency is : 6.81498e+06ns
----------- Synaptic Array w/o ADC (Forward + Activate Gradient) readLatency is : 1.43813e+07ns
----------- Buffer buffer latency is: 4.22737e+08ns
----------- Interconnect latency is: 4.68699e+07ns
----------- Weight Gradient Calculation readLatency is : 2.08155e+08ns
----------- Weight Update writeLatency is : 0ns
----------- DRAM data transfer Latency is : 225789ns
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
----------- ADC (or S/As and precharger for SRAM) readDynamicEnergy is : 2.74887e+08pJ
----------- Accumulation Circuits (subarray level: adders, shiftAdds; PE/Tile/Global level: accumulation units) readDynamicEnergy is : 5.91429e+08pJ
----------- Synaptic Array w/o ADC (Forward + Activate Gradient) readDynamicEnergy is : 1.98461e+08pJ
----------- Buffer readDynamicEnergy is: 1.55479e+08pJ
----------- Interconnect readDynamicEnergy is: 1.02385e+09pJ
----------- Weight Gradient Calculation readDynamicEnergy is : 2.52905e+09pJ
----------- Weight Update writeDynamicEnergy is : 2090.6pJ
----------- DRAM data transfer Energy is : 1.33848e+10pJ

************************ Breakdown of Latency and Dynamic Energy *************************

-------------------- Estimation of Layer 4 ----------------------
layer4's readLatency of Forward is: 2.37224e+07ns
layer4's readDynamicEnergy of Forward is: 8.53997e+07pJ
layer4's readLatency of Activation Gradient is: 2.40018e+07ns
layer4's readDynamicEnergy of Activation Gradient is: 8.65995e+07pJ
layer4's readLatency of Weight Gradient is: 8.39279e+07ns
layer4's readDynamicEnergy of Weight Gradient is: 6.36059e+10pJ
layer4's writeLatency of Weight Update is: 246870ns
layer4's writeDynamicEnergy of Weight Update is: 53143.2pJ
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
layer4's PEAK readLatency of Forward is: 1.27917e+07ns
layer4's PEAK readDynamicEnergy of Forward is: 4.0947e+07pJ
layer4's PEAK readLatency of Activation Gradient is: 1.30711e+07ns
layer4's PEAK readDynamicEnergy of Activation Gradient is: 4.21469e+07pJ
layer4's PEAK readLatency of Weight Gradient is: 4.20123e+06ns
layer4's PEAK readDynamicEnergy of Weight Gradient is: 5.8849e+08pJ
layer4's PEAK writeLatency of Weight Update is: 187751ns
layer4's PEAK writeDynamicEnergy of Weight Update is: 5265.28pJ
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
layer4's leakagePower is: 0.979497uW
layer4's leakageEnergy is: 186983pJ

************************ Breakdown of Latency and Dynamic Energy *************************

----------- ADC (or S/As and precharger for SRAM) readLatency is : 2.24e+07ns
----------- Accumulation Circuits (subarray level: adders, shiftAdds; PE/Tile/Global level: accumulation units) readLatency is : 1.2998e+06ns
----------- Synaptic Array w/o ADC (Forward + Activate Gradient) readLatency is : 2.16301e+06ns
----------- Buffer buffer latency is: 1.46573e+08ns
----------- Interconnect latency is: 1.24812e+06ns
----------- Weight Gradient Calculation readLatency is : 4.20123e+06ns
----------- Weight Update writeLatency is : 187751ns
----------- DRAM data transfer Latency is : 1.062e+06ns
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
----------- ADC (or S/As and precharger for SRAM) readDynamicEnergy is : 4.17359e+07pJ
----------- Accumulation Circuits (subarray level: adders, shiftAdds; PE/Tile/Global level: accumulation units) readDynamicEnergy is : 3.21112e+07pJ
----------- Synaptic Array w/o ADC (Forward + Activate Gradient) readDynamicEnergy is : 9.24685e+06pJ
----------- Buffer readDynamicEnergy is: 1.76234e+08pJ
----------- Interconnect readDynamicEnergy is: 3.00567e+07pJ
----------- Weight Gradient Calculation readDynamicEnergy is : 5.8849e+08pJ
----------- Weight Update writeDynamicEnergy is : 5265.28pJ
----------- DRAM data transfer Energy is : 6.29554e+10pJ

************************ Breakdown of Latency and Dynamic Energy *************************

-------------------- Estimation of Layer 5 ----------------------
layer5's readLatency of Forward is: 6.19007e+06ns
layer5's readDynamicEnergy of Forward is: 2.37084e+07pJ
layer5's readLatency of Activation Gradient is: 6.24742e+06ns
layer5's readDynamicEnergy of Activation Gradient is: 2.40591e+07pJ
layer5's readLatency of Weight Gradient is: 9.53493e+06ns
layer5's readDynamicEnergy of Weight Gradient is: 5.29526e+09pJ
layer5's writeLatency of Weight Update is: 4926.55ns
layer5's writeDynamicEnergy of Weight Update is: 9125.98pJ
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
layer5's PEAK readLatency of Forward is: 3.21043e+06ns
layer5's PEAK readDynamicEnergy of Forward is: 6.55577e+06pJ
layer5's PEAK readLatency of Activation Gradient is: 3.26778e+06ns
layer5's PEAK readDynamicEnergy of Activation Gradient is: 6.90644e+06pJ
layer5's PEAK readLatency of Weight Gradient is: 2.15838e+06ns
layer5's PEAK readDynamicEnergy of Weight Gradient is: 4.07341e+07pJ
layer5's PEAK writeLatency of Weight Update is: 0ns
layer5's PEAK writeDynamicEnergy of Weight Update is: 696.866pJ
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
layer5's leakagePower is: 0.979497uW
layer5's leakageEnergy is: 48730pJ

************************ Breakdown of Latency and Dynamic Energy *************************

----------- ADC (or S/As and precharger for SRAM) readLatency is : 5.6e+06ns
----------- Accumulation Circuits (subarray level: adders, shiftAdds; PE/Tile/Global level: accumulation units) readLatency is : 324951ns
----------- Synaptic Array w/o ADC (Forward + Activate Gradient) readLatency is : 553253ns
----------- Buffer buffer latency is: 1.681e+07ns
----------- Interconnect latency is: 378491ns
----------- Weight Gradient Calculation readLatency is : 2.15838e+06ns
----------- Weight Update writeLatency is : 0ns
----------- DRAM data transfer Latency is : 89368.4ns
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
----------- ADC (or S/As and precharger for SRAM) readDynamicEnergy is : 2.69098e+06pJ
----------- Accumulation Circuits (subarray level: adders, shiftAdds; PE/Tile/Global level: accumulation units) readDynamicEnergy is : 8.45401e+06pJ
----------- Synaptic Array w/o ADC (Forward + Activate Gradient) readDynamicEnergy is : 2.31723e+06pJ
----------- Buffer readDynamicEnergy is: 2.36278e+07pJ
----------- Interconnect readDynamicEnergy is: 1.49583e+07pJ
----------- Weight Gradient Calculation readDynamicEnergy is : 4.07341e+07pJ
----------- Weight Update writeDynamicEnergy is : 696.866pJ
----------- DRAM data transfer Energy is : 5.29776e+09pJ

************************ Breakdown of Latency and Dynamic Energy *************************

------------------------------ Summary --------------------------------

ChipArea : 4.86416e+06um^2
Chip total CIM (Forward+Activation Gradient) array : 15032.4um^2
Total IC Area on chip (Global and Tile/PE local): 910823um^2
Total ADC (or S/As and precharger for SRAM) Area on chip : 1.084e+06um^2
Total Accumulation Circuits (subarray level: adders, shiftAdds; PE/Tile/Global level: accumulation units) on chip : 861296um^2
Other Peripheries (e.g. decoders, mux, switchmatrix, buffers, pooling and activation units) : 645892um^2
Weight Gradient Calculation : 1.34711e+06um^2

-----------------------------------Chip layer-by-layer Estimation---------------------------------
Chip readLatency of Forward (per epoch) is: 4.41498e+09ns
Chip readDynamicEnergy of Forward (per epoch) is: 2.33148e+10pJ
Chip readLatency of Activation Gradient (per epoch) is: 1.16959e+09ns
Chip readDynamicEnergy of Activation Gradient (per epoch) is: 5.52861e+09pJ
Chip readLatency of Weight Gradient (per epoch) is: 5.65632e+09ns
Chip readDynamicEnergy of Weight Gradient (per epoch) is: 1.75498e+11pJ
Chip writeLatency of Weight Update (per epoch) is: 8.5627e+06ns
Chip writeDynamicEnergy of Weight Update (per epoch) is: 823929pJ
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Chip total Latency (per epoch) is: 1.12495e+10ns
Chip total Energy (per epoch) is: 2.04342e+11pJ
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Chip PEAK readLatency of Forward (per epoch) is: 8.83801e+08ns
Chip PEAK readDynamicEnergy of Forward (per epoch) is: 8.46333e+09pJ
Chip PEAK readLatency of Activation Gradient (per epoch) is: 2.67281e+08ns
Chip PEAK readDynamicEnergy of Activation Gradient (per epoch) is: 2.06452e+09pJ
Chip PEAK readLatency of Weight Gradient (per epoch) is: 4.78285e+09ns
Chip PEAK readDynamicEnergy of Weight Gradient (per epoch) is: 7.32989e+10pJ
Chip PEAK writeLatency of Weight Update (per epoch) is: 8.4408e+06ns
Chip PEAK writeDynamicEnergy of Weight Update (per epoch) is: 587707pJ
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Chip PEAK total Latency (per epoch) is: 5.94237e+09ns
Chip PEAK total Energy (per epoch) is: 8.38273e+10pJ
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Chip leakage Energy is: 6.38275e+07pJ
Chip leakage Power is: 11.7047uW

************************ Breakdown of Latency and Dynamic Energy *************************

----------- ADC (or S/As and precharger for SRAM) readLatency is : 9.968e+08ns
----------- Accumulation Circuits (subarray level: adders, shiftAdds; PE/Tile/Global level: accumulation units) readLatency is : 5.66061e+07ns
----------- Synaptic Array w/o ADC (Forward + Activate Gradient) readLatency is : 8.98542e+07ns
----------- Buffer readLatency is: 4.83362e+09ns
----------- Interconnect readLatency is: 5.69071e+08ns
----------- Weight Gradient Calculation readLatency is : 4.78285e+09ns
----------- Weight Update writeLatency is : 8.4408e+06ns
----------- DRAM data transfer Latency is : 2.50958e+06ns
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
----------- ADC (or S/As and precharger for SRAM) readDynamicEnergy is : 3.35745e+09pJ
----------- Accumulation Circuits (subarray level: adders, shiftAdds; PE/Tile/Global level: accumulation units) readDynamicEnergy is : 5.52208e+09pJ
----------- Synaptic Array w/o ADC (Forward + Activate Gradient) readDynamicEnergy is : 1.64832e+09pJ
----------- Buffer readDynamicEnergy is: 1.18989e+09pJ
----------- Interconnect readDynamicEnergy is: 1.29873e+10pJ
----------- Weight Gradient Calculation readDynamicEnergy is : 7.32989e+10pJ
----------- Weight Update writeDynamicEnergy is : 587707pJ
----------- DRAM data transfer DynamicEnergy is : 1.48768e+11pJ

************************ Breakdown of Latency and Dynamic Energy *************************


-----------------------------------Chip layer-by-layer Performance---------------------------------
Energy Efficiency TOPS/W: 1.22773
Throughput TOPS: 0.0223083
Throughput FPS: 0.0888932
--------------------------------------------------------------------------
Peak Energy Efficiency TOPS/W: 2.99373
Peak Throughput TOPS: 0.0422316
Peak Throughput FPS: 0.168283
-------------------------------------- Hardware Performance Done --------------------------------------

------------------------------ Simulation Performance --------------------------------
Total Run-time of NeuroSim: 0 seconds
------------------------------ Simulation Performance --------------------------------
Total Elapse: 374.12, Best Result: 17.160%
